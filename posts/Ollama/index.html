<!doctype html><html lang="en" ><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta name="theme-color" media="(prefers-color-scheme: light)" content="#f7f7f7"><meta name="theme-color" media="(prefers-color-scheme: dark)" content="#1b1b1e"><meta name="mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-status-bar-style" content="black-translucent"><meta name="viewport" content="width=device-width, user-scalable=no initial-scale=1, shrink-to-fit=no, viewport-fit=cover" ><meta name="generator" content="Jekyll v4.4.1" /><meta property="og:title" content="Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn" /><meta name="author" content="Ujwal Iyer" /><meta property="og:locale" content="en" /><meta name="description" content="A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy." /><meta property="og:description" content="A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy." /><link rel="canonical" href="https://ujwaliyer.com/posts/Ollama/" /><meta property="og:url" content="https://ujwaliyer.com/posts/Ollama/" /><meta property="og:site_name" content="Ujwal Iyer" /><meta property="og:type" content="article" /><meta property="article:published_time" content="2025-10-29T00:00:00+05:30" /><meta name="twitter:card" content="summary" /><meta property="twitter:title" content="Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn" /><meta name="twitter:site" content="@ujwaliyer" /><meta name="twitter:creator" content="@Ujwal Iyer" /> <script type="application/ld+json"> {"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Ujwal Iyer"},"dateModified":"2025-10-29T00:00:00+05:30","datePublished":"2025-10-29T00:00:00+05:30","description":"A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy.","headline":"Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn","mainEntityOfPage":{"@type":"WebPage","@id":"https://ujwaliyer.com/posts/Ollama/"},"url":"https://ujwaliyer.com/posts/Ollama/"}</script><title>Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn | Ujwal Iyer</title><link rel="icon" type="image/png" href="/assets/img/favicons/favicon-96x96.png" sizes="96x96"><link rel="icon" type="image/svg+xml" href="/assets/img/favicons/favicon.svg"><link rel="shortcut icon" href="/assets/img/favicons/favicon.ico"><link rel="apple-touch-icon" sizes="180x180" href="/assets/img/favicons/apple-touch-icon.png"><link rel="manifest" href="/assets/img/favicons/site.webmanifest"><link rel="preconnect" href="https://fonts.googleapis.com" ><link rel="dns-prefetch" href="https://fonts.googleapis.com" ><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><link rel="dns-prefetch" href="https://fonts.gstatic.com" ><link rel="preconnect" href="https://cdn.jsdelivr.net" ><link rel="dns-prefetch" href="https://cdn.jsdelivr.net" ><link rel="stylesheet" href="/assets/css/jekyll-theme-chirpy.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400&family=Source+Sans+Pro:wght@400;600;700;900&display=swap"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@7.1.0/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/tocbot@4.36.4/dist/tocbot.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/glightbox@3.3.0/dist/css/glightbox.min.css"> <script src="/assets/js/dist/theme.min.js"></script> <script defer src="https://cdn.jsdelivr.net/combine/npm/simple-jekyll-search@1.10.0/dest/simple-jekyll-search.min.js,npm/loading-attribute-polyfill@2.1.1/dist/loading-attribute-polyfill.umd.min.js,npm/glightbox@3.3.0/dist/js/glightbox.min.js,npm/clipboard@2.0.11/dist/clipboard.min.js,npm/dayjs@1.11.18/dayjs.min.js,npm/dayjs@1.11.18/locale/en.js,npm/dayjs@1.11.18/plugin/relativeTime.js,npm/dayjs@1.11.18/plugin/localizedFormat.js,npm/tocbot@4.36.4/dist/tocbot.min.js"></script> <script defer src="/assets/js/dist/post.min.js"></script> <script defer src="/app.min.js?baseurl=&register=true" ></script><body><aside aria-label="Sidebar" id="sidebar" class="d-flex flex-column align-items-end"><header class="profile-wrapper"> <a href="/" id="avatar" class="rounded-circle"></a> <a class="site-title d-block" href="/">Ujwal Iyer</a><p class="site-subtitle fst-italic mb-0">Learnings, Experiments & Notes"</p></header><nav class="flex-column flex-grow-1 w-100 ps-0"><ul class="nav"><li class="nav-item"> <a href="/" class="nav-link"> <i class="fa-fw fas fa-home"></i> <span>HOME</span> </a><li class="nav-item"> <a href="/categories/" class="nav-link"> <i class="fa-fw fas fa-stream"></i> <span>CATEGORIES</span> </a><li class="nav-item"> <a href="/tags/" class="nav-link"> <i class="fa-fw fas fa-tags"></i> <span>TAGS</span> </a><li class="nav-item"> <a href="/archives/" class="nav-link"> <i class="fa-fw fas fa-archive"></i> <span>ARCHIVES</span> </a><li class="nav-item"> <a href="/about/" class="nav-link"> <i class="fa-fw fas fa-info-circle"></i> <span>ABOUT</span> </a></ul></nav><div class="sidebar-bottom d-flex flex-wrap align-items-center w-100"> <button type="button" class="btn btn-link nav-link" aria-label="Switch Mode" id="mode-toggle"> <i class="fas fa-adjust"></i> </button> <span class="icon-border"></span> <a href="https://github.com/ujwaliyer" aria-label="github" target="_blank" rel="noopener noreferrer" > <i class="fab fa-github"></i> </a> <a href="https://twitter.com/ujwaliyer" aria-label="twitter" target="_blank" rel="noopener noreferrer" > <i class="fa-brands fa-x-twitter"></i> </a> <a href="javascript:location.href = 'mailto:' + ['ujwaliyer','live.com'].join('@')" aria-label="email" > <i class="fas fa-envelope"></i> </a> <a href="/feed.xml" aria-label="rss" > <i class="fas fa-rss"></i> </a></div></aside><div id="main-wrapper" class="d-flex justify-content-center"><div class="container d-flex flex-column px-xxl-5"><header id="topbar-wrapper" class="flex-shrink-0" aria-label="Top Bar"><div id="topbar" class="d-flex align-items-center justify-content-between px-lg-3 h-100" ><nav id="breadcrumb" aria-label="Breadcrumb"> <span> <a href="/">Home</a> </span> <span>Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</span></nav><button type="button" id="sidebar-trigger" class="btn btn-link" aria-label="Sidebar"> <i class="fas fa-bars fa-fw"></i> </button><div id="topbar-title"> Post</div><button type="button" id="search-trigger" class="btn btn-link" aria-label="Search"> <i class="fas fa-search fa-fw"></i> </button> <search id="search" class="align-items-center ms-3 ms-lg-0"> <i class="fas fa-search fa-fw"></i> <input class="form-control" id="search-input" type="search" aria-label="search" autocomplete="off" placeholder="Search..." > </search> <button type="button" class="btn btn-link text-decoration-none" id="search-cancel">Cancel</button></div></header><div class="row flex-grow-1"><main aria-label="Main Content" class="col-12 col-lg-11 col-xl-9 px-md-4"><article class="px-1" data-toc="true"><header><h1 data-toc-skip>Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</h1><p class="post-desc fw-light mb-4">A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy.</p><div class="post-meta text-muted"> <span> Posted <time data-ts="1761676200" data-df="ll" data-bs-toggle="tooltip" data-bs-placement="bottom" > Oct 29, 2025 </time> </span><div class="d-flex justify-content-between"> <span> By <em> </em> </span><div> <span class="readtime" data-bs-toggle="tooltip" data-bs-placement="bottom" title="746 words" > <em>4 min</em> read</span></div></div></div></header><div id="toc-bar" class="d-flex align-items-center justify-content-between invisible"> <span class="label text-truncate">Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</span> <button type="button" class="toc-trigger btn me-1"> <i class="fa-solid fa-list-ul fa-fw"></i> </button></div><button id="toc-solo-trigger" type="button" class="toc-trigger btn btn-outline-secondary btn-sm"> <span class="label ps-2 pe-1">Contents</span> <i class="fa-solid fa-angle-right fa-fw"></i> </button> <dialog id="toc-popup" class="p-0"><div class="header d-flex flex-row align-items-center justify-content-between"><div class="label text-truncate py-2 ms-4">Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</div><button id="toc-popup-close" type="button" class="btn mx-1 my-1 opacity-75"> <i class="fas fa-close"></i> </button></div><div id="toc-popup-content" class="px-4 py-3 pb-4"></div></dialog><div class="content"><p>When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that <em>just works</em>.</p><p>This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution.</p><hr /><h2 id="1-why-this-launch-matters"><span class="me-2">1. Why This Launch Matters</span><a href="#1-why-this-launch-matters" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos.<br /> Ollama changed that by shipping a <strong>self-contained LLM runtime</strong> that requires no Docker, no Python, no setup.</p><p>By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape.</p><hr /><h2 id="2-how-ollama-works-on-windows"><span class="me-2">2. How Ollama Works on Windows</span><a href="#2-how-ollama-works-on-windows" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><h3 id="a-the-runtime-core---go--llamacpp"><span class="me-2"><strong>a. The Runtime Core - Go + llama.cpp</strong></span><a href="#a-the-runtime-core---go--llamacpp" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Ollama’s runtime is written in Go, not Python or Node.<li>Go compiles into static binaries, bundling everything required to run locally - no external dependencies.<li>Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU.</ul><p>This combination lets users download a single <code class="language-plaintext highlighter-rouge">.exe</code> and start running models instantly - no environment setup, no path variables, no GPU driver hunting.</p><p>Abstraction is adoption. Reducing cognitive load isn’t just good UX - it’s a growth strategy.</p><hr /><h3 id="b-gpu-acceleration---directml-backend"><span class="me-2"><strong>b. GPU Acceleration - DirectML Backend</strong></span><a href="#b-gpu-acceleration---directml-backend" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12.<li>DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly.<li>If no compatible GPU exists, Ollama falls back gracefully to CPU inference.</ul><p><strong>Impact:</strong><br /> Every modern Windows machine becomes a viable LLM host - even without CUDA. PM lesson: build for the majority, not the elite hardware subset.</p><hr /><h3 id="c-unified-model-format---gguf"><span class="me-2"><strong>c. Unified Model Format - GGUF</strong></span><a href="#c-unified-model-format---gguf" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><ul><li>Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata.<li>The format supports memory mapping (MMAP) - only required layers are loaded into memory.<li>This is especially critical for Windows’ NTFS, which has higher IO overhead.</ul><p>Result: faster model load times, smaller memory footprint, and cleaner portability.</p><hr /><h3 id="d-local-openai-compatible-api-layer"><span class="me-2"><strong>d. Local OpenAI-Compatible API Layer</strong></span><a href="#d-local-openai-compatible-api-layer" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h3><p>One of Ollama’s smartest design moves lies in its API compatibility layer.<br /> It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route:</p><blockquote><p>POST /v1/chat/completions</p></blockquote><p>This endpoint behaves identically to OpenAI’s:</p><ul><li>Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream.<li>Streams tokens live via Server-Sent Events (SSE).<li>Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box.</ul><p>It also supports:</p><ul><li><code class="language-plaintext highlighter-rouge">/v1/completions</code> - for text-only generation<li><code class="language-plaintext highlighter-rouge">/v1/embeddings</code> - for RAG workflows<li><code class="language-plaintext highlighter-rouge">/api/generate</code> - Ollama’s original simple local route</ul><p>To switch any app from OpenAI to Ollama, simply change the base URL:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">OPENAI_API_BASE</span><span class="o">=</span><span class="s2">"http://localhost:11434/v1"</span>
<span class="nb">export </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">"ollama"</span>
</pre></table></code></div></div><p>Finally test locally:</p><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre><td class="rouge-code"><pre>curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Explain Ollama architecture"}]
  }'</span>
</pre></table></code></div></div><h2 id="3-setup-in-5-minutes"><span class="me-2">3. Setup in 5 Minutes</span><a href="#3-setup-in-5-minutes" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><ol><li>Download Ollama for Windows → <a href="https://ollama.ai/download">ollama.ai/download</a><li>Run the installer - it adds itself to PATH.<li>Pull your model:</ol><div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>   ollama pull llama3
</pre></table></code></div></div><ol><li>Run locally:<div class="language-bash highlighter-rouge"><div class="code-header"> <span data-label-text="Shell"><i class="fas fa-code fa-fw small"></i></span> <button aria-label="copy" data-title-succeed="Copied!"><i class="far fa-clipboard"></i></button></div><div class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre><td class="rouge-code"><pre>ollama run llama3
</pre></table></code></div></div><li>Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. Done. No Docker, no WSL, no Python virtualenvs.</ol><h2 id="4-key-takeaways-for-product-managers"><span class="me-2">4. Key Takeaways for Product Managers</span><a href="#4-key-takeaways-for-product-managers" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><div class="table-wrapper"><table><thead><tr><th>Theme<th>Insight<th>Example<tbody><tr><td>Reduce Friction<td>The simplest path to first success wins.<td>One-file install over multi-step setup.<tr><td>Leverage Existing Standards<td>Extend, don’t reinvent.<td>Full <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code> parity with OpenAI.<tr><td>Build for the Majority<td>Focus on mass-market hardware.<td>DirectML for GPU abstraction across vendors.<tr><td>Prioritize Reliability<td>Graceful fallbacks earn user trust.<td>Auto-switch to CPU when GPU unavailable.<tr><td>Ecosystem Thinking<td>APIs create flywheels.<td>Works natively with LangChain, Cursor, n8n.<tr><td>Invisible Architecture<td>Hide complexity behind defaults.<td>GGUF model format with MMAP.<tr><td>Empathy as Strategy<td>Developers don’t want power - they want flow.<td>“It just works” is the true retention loop.</table></div><blockquote><p>Adoption isn’t driven by novelty - it’s driven by effort reduction.</p></blockquote><h2 id="5-final-thoughts"><span class="me-2">5. Final Thoughts</span><a href="#5-final-thoughts" class="anchor text-muted"><i class="fas fa-hashtag"></i></a></h2><p>Ollama for Windows is a rare example where technical depth meets empathetic design. It redefined “local AI” from being an expert-only experiment to a mass-developer reality.</p><p>By blending:</p><ul><li>Go’s portability<li>DirectML’s universality<li>GGUF’s simplicity<li>OpenAI API compatibility</ul><blockquote><p>Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone.</p></blockquote></div><div class="post-tail-wrapper text-muted"><div class="post-tags"> <i class="fa fa-tags fa-fw me-1"></i> <a href="/tags/ai/" class="post-tag no-text-decoration" >AI</a> <a href="/tags/llm/" class="post-tag no-text-decoration" >LLM</a> <a href="/tags/product-management/" class="post-tag no-text-decoration" >Product Management</a> <a href="/tags/ollama/" class="post-tag no-text-decoration" >Ollama</a> <a href="/tags/windows/" class="post-tag no-text-decoration" >Windows</a> <a href="/tags/directml/" class="post-tag no-text-decoration" >DirectML</a> <a href="/tags/local-ai/" class="post-tag no-text-decoration" >Local AI</a> <a href="/tags/openai-api/" class="post-tag no-text-decoration" >OpenAI API</a></div><div class=" post-tail-bottom d-flex justify-content-between align-items-center mt-5 pb-2 " ><div class="license-wrapper"> This post is licensed under <a href="https://creativecommons.org/licenses/by/4.0/"> CC BY 4.0 </a> by the author.</div><div class="share-wrapper d-flex align-items-center"> <span class="share-label text-muted">Share</span> <span class="share-icons"> <a href="https://twitter.com/intent/tweet?text=Ollama%20for%20Windows:%20How%20Local%20LLMs%20Finally%20Work%20-%20And%20What%20Product%20Managers%20Can%20Learn%20-%20Ujwal%20Iyer&url=https%3A%2F%2Fujwaliyer.com%2Fposts%2FOllama%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Twitter" aria-label="Twitter"> <i class="fa-fw fa-brands fa-square-x-twitter"></i> </a> <a href="https://www.facebook.com/sharer/sharer.php?title=Ollama%20for%20Windows:%20How%20Local%20LLMs%20Finally%20Work%20-%20And%20What%20Product%20Managers%20Can%20Learn%20-%20Ujwal%20Iyer&u=https%3A%2F%2Fujwaliyer.com%2Fposts%2FOllama%2F" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Facebook" aria-label="Facebook"> <i class="fa-fw fab fa-facebook-square"></i> </a> <a href="https://t.me/share/url?url=https%3A%2F%2Fujwaliyer.com%2Fposts%2FOllama%2F&text=Ollama%20for%20Windows:%20How%20Local%20LLMs%20Finally%20Work%20-%20And%20What%20Product%20Managers%20Can%20Learn%20-%20Ujwal%20Iyer" target="_blank" rel="noopener" data-bs-toggle="tooltip" data-bs-placement="top" title="Telegram" aria-label="Telegram"> <i class="fa-fw fab fa-telegram"></i> </a> <button id="copy-link" aria-label="Copy link" class="btn small" data-bs-toggle="tooltip" data-bs-placement="top" title="Copy link" data-title-succeed="Link copied successfully!" > <i class="fa-fw fas fa-link pe-none fs-6"></i> </button> </span></div></div></div></article></main><aside aria-label="Panel" id="panel-wrapper" class="col-xl-3 ps-2 text-muted"><div class="access"><section id="access-lastmod"><h2 class="panel-heading">Recently Updated</h2><ul class="content list-unstyled ps-0 pb-1 ms-1 mt-2"><li class="text-truncate lh-lg"> <a href="/posts/AI-For-Kids/">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</a><li class="text-truncate lh-lg"> <a href="/posts/Ollama/">Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</a><li class="text-truncate lh-lg"> <a href="/posts/hello-world-ujwal-iyer/">Hello World - My Journey from Developer to Architect to Product Manager in today's age</a></ul></section><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">AI</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">LLM</a> <a class="post-tag btn btn-outline-primary" href="/tags/product-management/">Product Management</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-for-kids/">AI for kids</a> <a class="post-tag btn btn-outline-primary" href="/tags/aws/">AWS</a> <a class="post-tag btn btn-outline-primary" href="/tags/azure/">Azure</a> <a class="post-tag btn btn-outline-primary" href="/tags/career/">Career</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">Cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/curiosity/">curiosity</a> <a class="post-tag btn btn-outline-primary" href="/tags/directml/">DirectML</a></div></section></div><div class="toc-border-cover z-3"></div><section id="toc-wrapper" class="invisible position-sticky ps-0 pe-4 pb-4"><h2 class="panel-heading ps-3 pb-2 mb-0">Contents</h2><nav id="toc"></nav></section></aside></div><div class="row"><div id="tail-wrapper" class="col-12 col-lg-11 col-xl-9 px-md-4"><aside id="related-posts" aria-labelledby="related-label"><h3 class="mb-4" id="related-label">Further Reading</h3><nav class="row row-cols-1 row-cols-md-2 row-cols-xl-3 g-4 mb-4"><article class="col"> <a href="/posts/hello-world-ujwal-iyer/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1761589800" data-df="ll" > Oct 28, 2025 </time><h4 class="pt-0 my-2">Hello World - My Journey from Developer to Architect to Product Manager in today's age</h4><div class="text-muted"><p>I’m Ujwal Iyer - Senior Product Manager at SAP Labs India, combining deep technical roots in C#, cloud architecture, and SAP BTP with a passion for building impactful products.</p></div></div></a></article><article class="col"> <a href="/posts/AI-For-Kids/" class="post-preview card h-100"><div class="card-body"> <time data-ts="1761849000" data-df="ll" > Oct 31, 2025 </time><h4 class="pt-0 my-2">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</h4><div class="text-muted"><p>How a Dad, who's a Product Manager is learning AI hands-on by building a safe, voice-only LLM companion for his child - blending parenting, product thinking, and technology.</p></div></div></a></article></nav></aside><nav class="post-navigation d-flex justify-content-between" aria-label="Post Navigation"> <a href="/posts/hello-world-ujwal-iyer/" class="btn btn-outline-primary" aria-label="Older" ><p>Hello World - My Journey from Developer to Architect to Product Manager in today's age</p></a> <a href="/posts/AI-For-Kids/" class="btn btn-outline-primary" aria-label="Newer" ><p>Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</p></a></nav><footer aria-label="Site Info" class=" d-flex flex-column justify-content-center text-muted flex-lg-row justify-content-lg-between align-items-lg-center pb-lg-3 " ><p>© <time>2025</time> <a href="https://www.linkedin.com/in/ujwaliyer/">Ujwal Iyer</a>. <span data-bs-toggle="tooltip" data-bs-placement="top" title="Except where otherwise noted, the blog posts on this site are licensed under the Creative Commons Attribution 4.0 International (CC BY 4.0) License by the author." >Some rights reserved.</span></p><p>Using the <a data-bs-toggle="tooltip" data-bs-placement="top" title="v7.4.1" href="https://github.com/cotes2020/jekyll-theme-chirpy" target="_blank" rel="noopener" >Chirpy</a> theme for <a href="https://jekyllrb.com" target="_blank" rel="noopener">Jekyll</a>.</p></footer></div></div><div id="search-result-wrapper" class="d-flex justify-content-center d-none"><div class="col-11 content"><div id="search-hints"><section><h2 class="panel-heading">Trending Tags</h2><div class="d-flex flex-wrap mt-3 mb-1 me-3"> <a class="post-tag btn btn-outline-primary" href="/tags/ai/">AI</a> <a class="post-tag btn btn-outline-primary" href="/tags/llm/">LLM</a> <a class="post-tag btn btn-outline-primary" href="/tags/product-management/">Product Management</a> <a class="post-tag btn btn-outline-primary" href="/tags/ai-for-kids/">AI for kids</a> <a class="post-tag btn btn-outline-primary" href="/tags/aws/">AWS</a> <a class="post-tag btn btn-outline-primary" href="/tags/azure/">Azure</a> <a class="post-tag btn btn-outline-primary" href="/tags/career/">Career</a> <a class="post-tag btn btn-outline-primary" href="/tags/cloud/">Cloud</a> <a class="post-tag btn btn-outline-primary" href="/tags/curiosity/">curiosity</a> <a class="post-tag btn btn-outline-primary" href="/tags/directml/">DirectML</a></div></section></div><div id="search-results" class="d-flex flex-wrap justify-content-center text-muted mt-3"></div></div></div></div><aside aria-label="Scroll to Top"> <button id="back-to-top" type="button" class="btn btn-lg btn-box-shadow"> <i class="fas fa-angle-up"></i> </button></aside></div><div id="mask" class="d-none position-fixed w-100 h-100 z-1"></div><aside id="notification" class="toast" role="alert" aria-live="assertive" aria-atomic="true" data-bs-animation="true" data-bs-autohide="false" ><div class="toast-header"> <button type="button" class="btn-close ms-auto" data-bs-dismiss="toast" aria-label="Close" ></button></div><div class="toast-body text-center pt-0"><p class="px-2 mb-3">A new version of content is available.</p><button type="button" class="btn btn-primary" aria-label="Update"> Update </button></div></aside><script> document.addEventListener('DOMContentLoaded', () => { SimpleJekyllSearch({ searchInput: document.getElementById('search-input'), resultsContainer: document.getElementById('search-results'), json: '/assets/js/data/search.json', searchResultTemplate: '<article class="px-1 px-sm-2 px-lg-4 px-xl-0"><header><h2><a href="{url}">{title}</a></h2><div class="post-meta d-flex flex-column flex-sm-row text-muted mt-1 mb-1"> {categories} {tags}</div></header><p>{content}</p></article>', noResultsText: '<p class="mt-5">Oops! No results found.</p>', templateMiddleware: function(prop, value, template) { if (prop === 'categories') { if (value === '') { return `${value}`; } else { return `<div class="me-sm-4"><i class="far fa-folder fa-fw"></i>${value}</div>`; } } if (prop === 'tags') { if (value === '') { return `${value}`; } else { return `<div><i class="fa fa-tag fa-fw"></i>${value}</div>`; } } } }); }); </script>
