<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ujwaliyer.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ujwaliyer.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-26T00:15:59+05:30</updated><id>https://ujwaliyer.com/feed.xml</id><title type="html">Ujwal Iyer</title><subtitle>Learnings, Experiments &amp; Notes.</subtitle><entry><title type="html">A working raspberry pi voice chat bot version 1</title><link href="https://ujwaliyer.com/posts/VoiceChatbot-v1/" rel="alternate" type="text/html" title="A working raspberry pi voice chat bot version 1" /><published>2025-11-25T00:00:00+05:30</published><updated>2025-11-25T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/VoiceChatbot-v1</id><content type="html" xml:base="https://ujwaliyer.com/posts/VoiceChatbot-v1/"><![CDATA[<p>This post walks through the full system I built:</p>
<ul>
  <li>Architecture</li>
  <li>Libraries and tools</li>
  <li>How each service works</li>
  <li>The Ramayana RAG pipeline</li>
  <li>Key C# snippets that glue everything together</li>
</ul>

<div class="video-wide">
  <iframe src="https://www.youtube-nocookie.com/embed/-Sa_ptuP4ps?rel=0&amp;modestbranding=1" title="Hey Rama Demo v1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen="" loading="lazy">
  </iframe>
</div>

<h2 id="1-what-this-bot-actually-does">1. What this bot actually does</h2>

<p>As of today, my Kid Voice Bot can:</p>

<ul>
  <li>Record audio from a USB mic for a few seconds</li>
  <li>Transcribe it to text with Whisper</li>
  <li>Optionally pull context from a Ramayana PDF using a custom RAG index</li>
  <li>Ask Ollama (Llama 3.2 1B) for a short kid friendly answer</li>
  <li>Speak the answer using Piper TTS</li>
  <li>Play the audio out of speakers</li>
</ul>

<p>The whole thing is wrapped in a simple console UX:</p>

<ul>
  <li>Hit ENTER to record</li>
  <li>Bot answers in voice</li>
  <li>Type <code class="language-plaintext highlighter-rouge">/quit</code> to exit</li>
</ul>

<p>All core behaviour is config driven from <code class="language-plaintext highlighter-rouge">appsettings.json</code></p>

<h2 id="2-architecture">2. Architecture</h2>

<table>
  <thead>
    <tr>
      <th>Layer</th>
      <th>Technology</th>
      <th>Why</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>STT</strong></td>
      <td>Whisper (whisper.cpp via Whisper.Net)</td>
      <td>Fast, accurate, runs on Pi 5</td>
    </tr>
    <tr>
      <td><strong>LLM</strong></td>
      <td>Ollama (Llama 3.2 1B)</td>
      <td>Tight latency + high control</td>
    </tr>
    <tr>
      <td><strong>TTS</strong></td>
      <td>Piper</td>
      <td>Natural child-friendly voice, ultra fast</td>
    </tr>
    <tr>
      <td><strong>RAG</strong></td>
      <td>Embedding + cosine similarity + Ramayana PDF</td>
      <td>Cultural grounding</td>
    </tr>
    <tr>
      <td><strong>Audio</strong></td>
      <td>arecord + aplay</td>
      <td>Works well with ALSA on Raspberry Pi</td>
    </tr>
    <tr>
      <td><strong>Language</strong></td>
      <td>C# .NET 8</td>
      <td>Strong typing + easy modularization</td>
    </tr>
  </tbody>
</table>

<h3 id="logical-components">Logical Components</h3>

<p><img src="/assets/img/ProjectStructure.png" alt="" /></p>

<p>Audio layer</p>
<ul>
  <li>AudioRecorder for recording</li>
  <li>AudioPlayer for playback</li>
</ul>

<p>Model layer</p>
<ul>
  <li>WhisperTranscriber for speech to text</li>
  <li>OllamaClient for LLM calls</li>
  <li>PiperSpeaker for text to speech</li>
</ul>

<p>RAG layer</p>
<ul>
  <li>RamayanaIndexer to build a JSON index from a Ramayana PDF</li>
  <li>RamayanaRagService to fetch relevant chunks at runtime</li>
</ul>

<p>Orchestration layer</p>
<ul>
  <li>KidBot - the main loop that ties everything together</li>
  <li>Program - entry point and mode switch (index vs run)</li>
</ul>

<p>Configuration</p>
<ul>
  <li>appsettings.json and Config classes</li>
</ul>

<h2 id="3-configuration-driven-system">3. Configuration-Driven System</h2>

<p>Everything works off a single appsettings.json file. This makes testing/tuning easy.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
</pre></td><td class="rouge-code"><pre><span class="o">{</span>
  <span class="s2">"Audio"</span>: <span class="o">{</span>
    <span class="s2">"SampleRate"</span>: 16000,
    <span class="s2">"Channels"</span>: 1,
    <span class="s2">"SecondsPerClip"</span>: 4
  <span class="o">}</span>,
  <span class="s2">"Models"</span>: <span class="o">{</span>
    <span class="s2">"WhisperPath"</span>: <span class="s2">"/home/admin/models/whisper/ggml-base.en-q5_1.bin"</span>,
    <span class="s2">"PiperVoicePath"</span>: <span class="s2">"/home/admin/models/piper/en_US-amy-medium.onnx"</span>,
    <span class="s2">"PiperExe"</span>: <span class="s2">"/home/admin/.piper-venv/bin/piper"</span>
  <span class="o">}</span>,
  <span class="s2">"Ollama"</span>: <span class="o">{</span>
    <span class="s2">"BaseUrl"</span>: <span class="s2">"http://localhost:11434"</span>,
    <span class="s2">"Model"</span>: <span class="s2">"llama3.2:1b"</span>,
    <span class="s2">"SystemPrompt"</span>: <span class="s2">"You are a friendly, kid-safe assistant. Use very simple words, 2-3 short sentences."</span>
  <span class="o">}</span>,
  <span class="s2">"Rag"</span>: <span class="o">{</span>
    <span class="s2">"Enabled"</span>: <span class="nb">true</span>,
    <span class="s2">"QdrantEndpoint"</span>: <span class="s2">"http://localhost:6333"</span>,
    <span class="s2">"CollectionName"</span>: <span class="s2">"ramayana"</span>,
    <span class="s2">"EmbeddingModel"</span>: <span class="s2">"nomic-embed-text"</span>,
    <span class="s2">"TopK"</span>: 3
  <span class="o">}</span>,
  <span class="s2">"Ramayana"</span>: <span class="o">{</span>
    <span class="s2">"PdfPath"</span>: <span class="s2">"/home/admin/data/ramayana.pdf"</span>,
    <span class="s2">"ChunkCharacters"</span>: 900,
    <span class="s2">"IndexPath"</span>: <span class="s2">"/home/admin/data/ramayana_index.json"</span>
  <span class="o">}</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="4-audio-layer-recording--playback">4. Audio Layer (Recording + Playback)</h2>

<p>On the Pi I decided to keep it simple and lean on ALSA using lightweight process wrappers inside the AudioServices.cs class. ALSA is the lowest level of the Linux sound stack, and can be access through the CLI.</p>

<p>Recording (AudioRecorder)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
</pre></td><td class="rouge-code"><pre>public static async Task&lt;string&gt; RecordWavAsync<span class="o">(</span>string path, AudioConfig cfg<span class="o">)</span>
<span class="o">{</span>
    var args <span class="o">=</span> <span class="s2">$"-f S16_LE -r {cfg.SampleRate} -c {cfg.Channels} "</span> +
               <span class="s2">$"-d {cfg.SecondsPerClip} -t wav -q </span><span class="se">\"</span><span class="s2">{path}</span><span class="se">\"</span><span class="s2">"</span><span class="p">;</span>

    var psi <span class="o">=</span> new ProcessStartInfo<span class="o">(</span><span class="s2">"arecord"</span>, args<span class="o">)</span>
    <span class="o">{</span>
        RedirectStandardError <span class="o">=</span> <span class="nb">true</span>,
        RedirectStandardOutput <span class="o">=</span> <span class="nb">true</span>
    <span class="o">}</span><span class="p">;</span>

    using var p <span class="o">=</span> Process.Start<span class="o">(</span>psi<span class="o">)</span><span class="p">;</span>
    <span class="k">if</span> <span class="o">(</span>p <span class="o">==</span> null<span class="o">)</span>
        throw new InvalidOperationException<span class="o">(</span><span class="s2">"Failed to start 'arecord'."</span><span class="o">)</span><span class="p">;</span>

    await p.WaitForExitAsync<span class="o">()</span><span class="p">;</span>
    <span class="k">if</span> <span class="o">(</span>p.ExitCode <span class="o">!=</span> 0<span class="o">)</span>
    <span class="o">{</span>
        var err <span class="o">=</span> await p.StandardError.ReadToEndAsync<span class="o">()</span><span class="p">;</span>
        throw new InvalidOperationException<span class="o">(</span><span class="s2">$"arecord failed: {err}"</span><span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>

    <span class="k">return </span>path<span class="p">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Playback (AudioPlayer)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
</pre></td><td class="rouge-code"><pre>public static async Task PlayWavAsync<span class="o">(</span>string wavPath<span class="o">)</span>
<span class="o">{</span>
    var psi <span class="o">=</span> new ProcessStartInfo<span class="o">(</span><span class="s2">"aplay"</span>, <span class="s2">$"</span><span class="se">\"</span><span class="s2">{wavPath}</span><span class="se">\"</span><span class="s2">"</span><span class="o">)</span>
    <span class="o">{</span>
        RedirectStandardError <span class="o">=</span> <span class="nb">true</span>,
        RedirectStandardOutput <span class="o">=</span> <span class="nb">true</span>
    <span class="o">}</span><span class="p">;</span>

    using var p <span class="o">=</span> Process.Start<span class="o">(</span>psi<span class="o">)</span><span class="p">;</span>
    <span class="k">if</span> <span class="o">(</span>p <span class="o">==</span> null<span class="o">)</span>
    <span class="o">{</span>
        Console.WriteLine<span class="o">(</span><span class="s2">"Could not start 'aplay'."</span><span class="o">)</span><span class="p">;</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="o">}</span>

    await p.WaitForExitAsync<span class="o">()</span><span class="p">;</span>
    <span class="k">if</span> <span class="o">(</span>p.ExitCode <span class="o">!=</span> 0<span class="o">)</span>
    <span class="o">{</span>
        var err <span class="o">=</span> await p.StandardError.ReadToEndAsync<span class="o">()</span><span class="p">;</span>
        Console.WriteLine<span class="o">(</span><span class="s2">$"aplay error: {err}"</span><span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="5-whisper-stt-integration">5. Whisper STT Integration</h2>

<p>For STT, I used Whisper.Net, which wraps whisper.cpp nicely for C#. I load the GGML model from the config path.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre>public WhisperTranscriber<span class="o">(</span>string modelPath<span class="o">)</span>
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span>string.IsNullOrWhiteSpace<span class="o">(</span>modelPath<span class="o">))</span>
        throw new ArgumentException<span class="o">(</span><span class="s2">"Whisper model path is not configured."</span>, nameof<span class="o">(</span>modelPath<span class="o">))</span><span class="p">;</span>

    _factory <span class="o">=</span> WhisperFactory.FromPath<span class="o">(</span>modelPath<span class="o">)</span><span class="p">;</span>
    _processor <span class="o">=</span> _factory.CreateBuilder<span class="o">()</span>
        .WithLanguage<span class="o">(</span><span class="s2">"en"</span><span class="o">)</span>
        .Build<span class="o">()</span><span class="p">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>running the actual transcription below, which gives me plain text from the child’s speech. I keep it raw and simple and handle grammar problems at the LLM layer.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
</pre></td><td class="rouge-code"><pre>public async Task&lt;string&gt; TranscribeAsync<span class="o">(</span>string wavPath<span class="o">)</span>
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(!</span>File.Exists<span class="o">(</span>wavPath<span class="o">))</span>
        throw new FileNotFoundException<span class="o">(</span><span class="s2">"Audio file not found"</span>, wavPath<span class="o">)</span><span class="p">;</span>

    var sb <span class="o">=</span> new StringBuilder<span class="o">()</span><span class="p">;</span>

    await using var fs <span class="o">=</span> File.OpenRead<span class="o">(</span>wavPath<span class="o">)</span><span class="p">;</span>
    await foreach <span class="o">(</span>var segment <span class="k">in </span>_processor.ProcessAsync<span class="o">(</span>fs<span class="o">))</span>
    <span class="o">{</span>
        sb.Append<span class="o">(</span>segment.Text<span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>

    <span class="k">return </span>sb.ToString<span class="o">()</span>.Trim<span class="o">()</span><span class="p">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="6-piper-tts--speaking-back-to-the-child">6. Piper TTS- speaking back to the child</h2>

<p>Piper is called as an external process as well. Again, small wrapper, clear contract.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
</pre></td><td class="rouge-code"><pre>public PiperSpeaker<span class="o">(</span>ModelsConfig models<span class="o">)</span>
<span class="o">{</span>
    _exe <span class="o">=</span> string.IsNullOrWhiteSpace<span class="o">(</span>models.PiperExe<span class="o">)</span>
        ? <span class="s2">"/usr/bin/piper"</span>
        : models.PiperExe<span class="p">;</span>

    _modelPath <span class="o">=</span> models.PiperVoicePath<span class="p">;</span>

    <span class="k">if</span> <span class="o">(</span>string.IsNullOrWhiteSpace<span class="o">(</span>_modelPath<span class="o">))</span>
        throw new ArgumentException<span class="o">(</span><span class="s2">"Piper voice model path is not configured."</span>, nameof<span class="o">(</span>models.PiperVoicePath<span class="o">))</span><span class="p">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>
<p>The actual speech:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
</pre></td><td class="rouge-code"><pre>public async Task SpeakAsync<span class="o">(</span>string text, string wavOutPath<span class="o">)</span>
<span class="o">{</span>
    <span class="k">if</span> <span class="o">(</span>string.IsNullOrWhiteSpace<span class="o">(</span>text<span class="o">))</span>
    <span class="o">{</span>
        Console.WriteLine<span class="o">(</span><span class="s2">"[TTS] Empty text. Skipping Piper."</span><span class="o">)</span><span class="p">;</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="o">}</span>

    var psi <span class="o">=</span> new ProcessStartInfo<span class="o">(</span>_exe<span class="o">)</span>
    <span class="o">{</span>
        RedirectStandardInput <span class="o">=</span> <span class="nb">true</span>,
        RedirectStandardOutput <span class="o">=</span> <span class="nb">true</span>,
        RedirectStandardError <span class="o">=</span> <span class="nb">true</span>,
        UseShellExecute <span class="o">=</span> <span class="nb">false</span>
    <span class="o">}</span><span class="p">;</span>

    psi.ArgumentList.Add<span class="o">(</span><span class="s2">"-m"</span><span class="o">)</span><span class="p">;</span>
    psi.ArgumentList.Add<span class="o">(</span>_modelPath<span class="o">)</span><span class="p">;</span>
    psi.ArgumentList.Add<span class="o">(</span><span class="s2">"-f"</span><span class="o">)</span><span class="p">;</span>
    psi.ArgumentList.Add<span class="o">(</span>wavOutPath<span class="o">)</span><span class="p">;</span>

    using var proc <span class="o">=</span> Process.Start<span class="o">(</span>psi<span class="o">)</span><span class="p">;</span>
    <span class="k">if</span> <span class="o">(</span>proc <span class="o">==</span> null<span class="o">)</span>
        throw new InvalidOperationException<span class="o">(</span><span class="s2">"Failed to start Piper."</span><span class="o">)</span><span class="p">;</span>

    await proc.StandardInput.WriteLineAsync<span class="o">(</span>text<span class="o">)</span><span class="p">;</span>
    await proc.StandardInput.FlushAsync<span class="o">()</span><span class="p">;</span>
    proc.StandardInput.Close<span class="o">()</span><span class="p">;</span>

    var stderrTask <span class="o">=</span> proc.StandardError.ReadToEndAsync<span class="o">()</span><span class="p">;</span>
    var stdoutTask <span class="o">=</span> proc.StandardOutput.ReadToEndAsync<span class="o">()</span><span class="p">;</span>

    await proc.WaitForExitAsync<span class="o">()</span><span class="p">;</span>
    var stderr <span class="o">=</span> await stderrTask<span class="p">;</span>
    var stdout <span class="o">=</span> await stdoutTask<span class="p">;</span>

    <span class="k">if</span> <span class="o">(</span>proc.ExitCode <span class="o">!=</span> 0<span class="o">)</span>
    <span class="o">{</span>
        throw new InvalidOperationException<span class="o">(</span>
            <span class="s2">$"Piper exited with code {proc.ExitCode}.</span><span class="se">\n</span><span class="s2">STDERR:</span><span class="se">\n</span><span class="s2">{stderr}</span><span class="se">\n</span><span class="s2">STDOUT:</span><span class="se">\n</span><span class="s2">{stdout}"</span><span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="7-ollama-llm-client--prompting-strategy">7. Ollama LLM Client + Prompting Strategy</h2>

<p>Prompt constructed dynamically with system prompt + RAG context + child’s query.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
32
33
34
35
36
37
38
39
40
41
42
43
44
45
46
</pre></td><td class="rouge-code"><pre>public async Task&lt;string&gt; GenerateReplyAsync<span class="o">(</span>
    string userText, 
    string? context <span class="o">=</span> null,
    CancellationToken ct <span class="o">=</span> default<span class="o">)</span>
<span class="o">{</span>
    var sb <span class="o">=</span> new StringBuilder<span class="o">()</span><span class="p">;</span>
    sb.AppendLine<span class="o">(</span>_cfg.SystemPrompt<span class="o">)</span><span class="p">;</span>
    sb.AppendLine<span class="o">()</span><span class="p">;</span>

    <span class="k">if</span> <span class="o">(!</span>string.IsNullOrWhiteSpace<span class="o">(</span>context<span class="o">))</span>
    <span class="o">{</span>
        sb.AppendLine<span class="o">(</span><span class="s2">"Here are some passages from the Ramayana. Use them to answer:"</span><span class="o">)</span><span class="p">;</span>
        sb.AppendLine<span class="o">(</span>context<span class="o">)</span><span class="p">;</span>
        sb.AppendLine<span class="o">()</span><span class="p">;</span>
    <span class="o">}</span>

    sb.AppendLine<span class="o">(</span><span class="s2">$"Child said: </span><span class="se">\"</span><span class="s2">{userText}</span><span class="se">\"</span><span class="s2">"</span><span class="o">)</span><span class="p">;</span>
    sb.AppendLine<span class="o">()</span><span class="p">;</span>
    sb.AppendLine<span class="o">(</span><span class="s2">"Answer in 3-4 simple sentences for a child."</span><span class="o">)</span><span class="p">;</span>

    var reqObj <span class="o">=</span> new
    <span class="o">{</span>
        model <span class="o">=</span> _cfg.Model,
        prompt <span class="o">=</span> sb.ToString<span class="o">()</span>,
        stream <span class="o">=</span> <span class="nb">false</span>,
        keep_alive <span class="o">=</span> 300
    <span class="o">}</span><span class="p">;</span>

    var json <span class="o">=</span> JsonSerializer.Serialize<span class="o">(</span>reqObj<span class="o">)</span><span class="p">;</span>
    using var content <span class="o">=</span> new StringContent<span class="o">(</span>json, Encoding.UTF8, <span class="s2">"application/json"</span><span class="o">)</span><span class="p">;</span>
    var url <span class="o">=</span> <span class="s2">$"{_cfg.BaseUrl.TrimEnd('/')}/api/generate"</span><span class="p">;</span>

    HttpResponseMessage resp<span class="p">;</span>
    try
    <span class="o">{</span>
        resp <span class="o">=</span> await Http.PostAsync<span class="o">(</span>url, content, ct<span class="o">)</span><span class="p">;</span>
    <span class="o">}</span>
    catch <span class="o">(</span>Exception ex<span class="o">)</span>
    <span class="o">{</span>
        Console.WriteLine<span class="o">(</span><span class="s2">$"[LLM] HTTP error: {ex.Message}"</span><span class="o">)</span><span class="p">;</span>
        <span class="k">return </span>string.Empty<span class="p">;</span>
    <span class="o">}</span>

    var body <span class="o">=</span> await resp.Content.ReadAsStringAsync<span class="o">(</span>ct<span class="o">)</span><span class="p">;</span>
    ...
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="8-ramayana-rag">8. Ramayana RAG</h2>

<p>I wanted my child to hear stories and answers rooted in the Ramayana (retelling by C Rajagopalachari), not random internet style text. So I implemented a custom RAG pipeline in 2 parts:</p>
<ul>
  <li>Indexer that runs once and creates a JSON index from the Ramayana PDF</li>
  <li>Runtime service that looks up relevant chunks per question</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
</pre></td><td class="rouge-code"><pre>using var doc <span class="o">=</span> PdfDocument.Open<span class="o">(</span>pdfPath<span class="o">)</span><span class="p">;</span>
foreach <span class="o">(</span>Page page <span class="k">in </span>doc.GetPages<span class="o">())</span> <span class="o">{</span> ... <span class="o">}</span>

//Chunking
<span class="k">if</span> <span class="o">(</span>sb.Length <span class="o">&gt;=</span> chunkChars<span class="o">)</span>
    chunks.Add<span class="o">(</span>new TextChunk <span class="o">{</span> Text <span class="o">=</span> sb.ToString<span class="o">()</span>.Trim<span class="o">()</span>, ... <span class="o">})</span><span class="p">;</span>

//Embedding via Ollama:
var req <span class="o">=</span> new <span class="o">{</span> model <span class="o">=</span> cfg.Rag.EmbeddingModel, input <span class="o">=</span> text <span class="o">}</span><span class="p">;</span>
var resp <span class="o">=</span> await Http.PostAsync<span class="o">(</span>url, content, ct<span class="o">)</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Full index persisted as JSON to the disk. At runtime, RamayanaRagService loads the JSON index and handles all similarity search.</p>

<h2 id="9-kidbot-orchestration-layer">9. KidBot Orchestration Layer</h2>

<p>This is the glue that ties everything together.</p>
<ul>
  <li>Wait for ENTER</li>
  <li>Record audio</li>
  <li>Transcribe</li>
  <li>Run RAG</li>
  <li>Generate LLM reply</li>
  <li>Speak via Piper</li>
  <li>Playback</li>
  <li>Repeat</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>clipPath <span class="o">=</span> await AudioRecorder.RecordWavAsync<span class="o">(</span><span class="s2">"clip.wav"</span>, _config.Audio<span class="o">)</span><span class="p">;</span>
text <span class="o">=</span> await _stt.TranscribeAsync<span class="o">(</span>clipPath<span class="o">)</span><span class="p">;</span>
context <span class="o">=</span> await _rag.TryGetContextAsync<span class="o">(</span>text<span class="o">)</span><span class="p">;</span>
reply <span class="o">=</span> await _llm.GenerateReplyAsync<span class="o">(</span>text, context<span class="o">)</span><span class="p">;</span>
await _tts.SpeakAsync<span class="o">(</span>reply, <span class="s2">"reply.wav"</span><span class="o">)</span><span class="p">;</span>
await AudioPlayer.PlayWavAsync<span class="o">(</span><span class="s2">"reply.wav"</span><span class="o">)</span><span class="p">;</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="10-entry-point-and-modes">10. Entry point and modes</h2>

<p>The Program class sets up dependency wiring and supports two modes out of the box:</p>
<ul>
  <li>index - build or rebuild the Ramayana index</li>
  <li>default - run the interactive voice bot</li>
</ul>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
31
</pre></td><td class="rouge-code"><pre>static async Task Main<span class="o">(</span>string[] args<span class="o">)</span>
<span class="o">{</span>
    var configurationRoot <span class="o">=</span> new ConfigurationBuilder<span class="o">()</span>
        .SetBasePath<span class="o">(</span>Directory.GetCurrentDirectory<span class="o">())</span>
        .AddJsonFile<span class="o">(</span><span class="s2">"appsettings.json"</span>, optional: <span class="nb">false</span><span class="o">)</span>
        .Build<span class="o">()</span><span class="p">;</span>

    var config <span class="o">=</span> configurationRoot.Get&lt;Config&gt;<span class="o">()</span>
                 ?? throw new InvalidOperationException<span class="o">(</span><span class="s2">"Failed to bind appsettings.json to Config."</span><span class="o">)</span><span class="p">;</span>

    // Index mode
    <span class="k">if</span> <span class="o">(</span>args.Length <span class="o">&gt;</span> 0 <span class="o">&amp;&amp;</span> args[0].Equals<span class="o">(</span><span class="s2">"index"</span>, StringComparison.OrdinalIgnoreCase<span class="o">))</span>
    <span class="o">{</span>
        Console.WriteLine<span class="o">(</span><span class="s2">"Running Ramayana indexer..."</span><span class="o">)</span><span class="p">;</span>
        await RamayanaIndexer.BuildIndexAsync<span class="o">(</span>config<span class="o">)</span><span class="p">;</span>
        <span class="k">return</span><span class="p">;</span>
    <span class="o">}</span>

    Console.WriteLine<span class="o">(</span><span class="s2">"Starting Whisper (STT)..."</span><span class="o">)</span><span class="p">;</span>
    await using var whisper <span class="o">=</span> new WhisperTranscriber<span class="o">(</span>config.Models.WhisperPath<span class="o">)</span><span class="p">;</span>

    Console.WriteLine<span class="o">(</span><span class="s2">"Starting Ollama client..."</span><span class="o">)</span><span class="p">;</span>
    using var llm <span class="o">=</span> new OllamaClient<span class="o">(</span>config.Ollama<span class="o">)</span><span class="p">;</span>

    Console.WriteLine<span class="o">(</span><span class="s2">"Starting Piper (TTS)..."</span><span class="o">)</span><span class="p">;</span>
    var tts <span class="o">=</span> new PiperSpeaker<span class="o">(</span>config.Models<span class="o">)</span><span class="p">;</span>

    var rag <span class="o">=</span> new RamayanaRagService<span class="o">(</span>config<span class="o">)</span><span class="p">;</span>
    var bot <span class="o">=</span> new KidBot<span class="o">(</span>config, whisper, llm, tts, rag<span class="o">)</span><span class="p">;</span>
    await bot.RunAsync<span class="o">()</span><span class="p">;</span>
<span class="o">}</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="11-whats-next-roadmap">11. What’s Next (Roadmap)</h2>

<h3 id="audio-layer">Audio layer</h3>
<ul>
  <li>Add VAD (voice activity detection) to stop recording once silence is detected. This directly reduces Whisper processing time.</li>
  <li>Instead of writing audio to disk and reading again, capture into memory</li>
  <li>Add wake word (“Hey Rama”)</li>
  <li>Keep Piper process warm- reduce per process startup cost</li>
</ul>

<h3 id="embeddings">Embeddings</h3>
<ul>
  <li>Better quality chunking: Right now it’s fixed character chunking. Upgrade to semantic chunking with overlap using Qdrant</li>
</ul>

<h3 id="llm">LLM</h3>
<ul>
  <li>Limit tokens generated (num_predict)</li>
  <li>Shorter system prompt + context: Keep system prompt extremely tight and trim from topk from 5 to 2.</li>
  <li>Ollama is loading/unloading the model each time: Add “keep_alive”: 300 to ensure the model is kept warm between queries.</li>
</ul>

<h3 id="product-experience">Product Experience</h3>
<ul>
  <li>Session memory per conversation</li>
  <li>Safer guardrails using LlamaGuard</li>
  <li>Add more RAG content- mahabharata, story teller, math, nature, space, etc</li>
  <li>Add Agentic RAG layer using Microsoft Semantic Kernel, so requests are directly matched with the right collections in Qdrant</li>
  <li>Graceful degradation- if RAG context is null, have a fallback answer</li>
</ul>]]></content><author><name></name></author><category term="Raspberry Pi" /><category term="HAT" /><category term="NVMe SSD" /><summary type="html"><![CDATA[First version of a working and offline voice bot]]></summary></entry><entry><title type="html">Unpacking toddler persona &amp;amp; AI-ML solutions for an AI Voice Bot</title><link href="https://ujwaliyer.com/posts/ToddlerPersonaResearch-AI-Solutions/" rel="alternate" type="text/html" title="Unpacking toddler persona &amp;amp; AI-ML solutions for an AI Voice Bot" /><published>2025-11-06T00:00:00+05:30</published><updated>2025-11-06T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/ToddlerPersonaResearch-AI-Solutions</id><content type="html" xml:base="https://ujwaliyer.com/posts/ToddlerPersonaResearch-AI-Solutions/"><![CDATA[<p>As a Senior Product Manager, I’ve spent years obsessing over user personas, friction points, and success metrics. But nothing prepared me for my newest and toughest customer: my five-year-old toddler.</p>

<p>He doesn’t just ask questions. He fires rapid, chaotic queries at bedtime.<br />
“Appa, why Rama go to forest?” (where to start- because his Dasharatha told him so, or because he promised Kaikeyi that he will grant her boon when the time came?)
“Why did Krishna help Arjuna but not Duryodhana?” (tough to explain righteousness/dharma)<br />
“Why gas fire blue but diya fire red?” (I still don’t know!)</p>

<p>This blog will focus on pure user research, and suggest AI frameworks and subsequent solutions of the pain point faced by me.</p>

<hr />

<h2 id="1-persona-deep-dive-the-curious-toddler">1. Persona Deep Dive: “The Curious toddler”</h2>

<h3 id="who-he-is">Who he is</h3>
<ul>
  <li>Age: 5 years</li>
  <li>Context: bilingual (English + Tamil)</li>
  <li>Hardware: Raspberry Pi 5, 16GB</li>
  <li>Interface: Voice only - no screen, no keyboard</li>
</ul>

<h3 id="behavior-traits">Behavior Traits</h3>
<ul>
  <li>Speaks in short, broken english</li>
  <li>Has a 20-second patience window hence easily distracted - responses longer than 20 seconds are lost</li>
  <li>Emotionally reactive - cannot have wrong tone or scary words</li>
</ul>

<hr />

<h2 id="2-jobs-to-be-done--paingain-matrix">2. Jobs-to-be-Done + Pain/Gain Matrix</h2>

<table>
  <thead>
    <tr>
      <th>Job</th>
      <th>Current Workaround</th>
      <th>Pain (1–5)</th>
      <th>Gain if Solved (1–5)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Understand epic stories</td>
      <td>Parents explain</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Ask “why” questions safely</td>
      <td>YT Kids/Khan Academy</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Stay engaged for short bursts</td>
      <td>Cartoons</td>
      <td>5</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Speak naturally</td>
      <td>Speech-to-text confusion</td>
      <td>4</td>
      <td>5</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="3-friction-heatmap">3. Friction Heatmap</h2>

<table>
  <thead>
    <tr>
      <th>Stage</th>
      <th>Friction</th>
      <th>Severity (1–5)</th>
      <th>Fix Priority</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Voice to Text</td>
      <td>Broken grammar</td>
      <td>5</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Query Understanding</td>
      <td>Needs semantic rewriting</td>
      <td>5</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Answer Generation</td>
      <td>Too long / too abstract</td>
      <td>4</td>
      <td>High</td>
    </tr>
    <tr>
      <td>Response Filtering</td>
      <td>Unsafe/adult/violent content</td>
      <td>5</td>
      <td>Critical</td>
    </tr>
    <tr>
      <td>Text-to-Speech</td>
      <td>Must sound friendly &amp; child-like</td>
      <td>3</td>
      <td>Medium</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="4-success-metrics">4. Success Metrics</h2>

<ul>
  <li>Attention Retention: responses under 20 seconds</li>
  <li>Understanding Score: answers correctly paraphrased by child</li>
  <li>Safe Response Ratio: age-safe answers</li>
  <li>Latency: TTS response delivered under 2 seconds</li>
</ul>

<hr />

<h2 id="5-technical-blueprint-ai-concepts-to-solve-friction--jobs-to-be-done">5. Technical Blueprint: AI Concepts to solve Friction &amp; Jobs to be done</h2>

<p>Here’s the system I designed for now:</p>

<h4 id="speech-to-text-cleanup">Speech-to-Text Cleanup</h4>
<ul>
  <li>Handled by: SymSpell</li>
  <li>Why: Fast spelling and grammar correction</li>
</ul>

<h4 id="prompt-rewriting">Prompt Rewriting</h4>
<ul>
  <li>Handled by: Llama 3.1 7B (Quantized)</li>
  <li>Why: Reformulates prompts so chunked vector matches correctly</li>
</ul>

<h4 id="vectorize-content">Vectorize Content</h4>
<ul>
  <li>Handled by: ONNX MiniLM</li>
  <li>Why: Transformer-based embeddings</li>
</ul>

<h4 id="storage--retrieval">Storage / Retrieval</h4>
<ul>
  <li>Handled by: Qdrant</li>
  <li>Why: Scalable vector DB and similarity search</li>
</ul>

<h4 id="answer-generation">Answer Generation</h4>
<ul>
  <li>Handled by: Llama 3.1 7B (again)</li>
  <li>Why: Generates short, story-like responses</li>
</ul>

<h4 id="kiddy-guardrails">Kiddy Guardrails</h4>
<ul>
  <li>Handled by: LlamaGuard</li>
  <li>Why: Filters complex or violent content before TTS</li>
</ul>

<h4 id="text-to-speech-tts">Text-to-Speech (TTS)</h4>
<ul>
  <li>Handled by: Coqui TTS or System.Speech</li>
  <li>Why: Produces warm, human-like voice output</li>
</ul>

<p>Each component runs locally on the Raspberry Pi, respecting privacy and keeping everything offline.<br />
No cloud calls, no hidden data drift - just pure, safe, curious AI for a curious child.</p>

<hr />

<h2 id="6-why-net-makes-it-possible">6. Why .NET Makes It Possible</h2>

<p>I chose .NET not just out of nostalgia, but also because the AI ecosystem quietly supports all of this:</p>

<ul>
  <li>STT + TTS → System.Speech namespace or Coqui integration via C#</li>
  <li>Qdrant Vector DB → Official .NET client with REST bindings</li>
  <li>ONNX Runtime → Runs MiniLM embeddings natively on CPU</li>
  <li>Llama 3.1 7B (GGUF) → Compatible via llama.cpp bindings for .NET</li>
  <li>SymSpell.NET → Lightweight typo and grammar fixer</li>
  <li>LLamaSharp → C# binding for llama.cpp so can load LlamaGuard model</li>
</ul>

<hr />

<h3 id="watch-this-space">Watch this space</h3>
<p>I’ll be sharing the step-by-step build of this <strong>AI Toddler Explorer</strong> soon - from RAG pipelines to voice tuning - all on a humble Raspberry Pi 5.<br />
Follow my journey at <a href="https://ujwaliyer.com">ujwaliyer.com</a> - where bedtime questions meet machine learning.</p>]]></content><author><name></name></author><category term="AI Projects" /><category term="Product Management" /><category term="Raspberry Pi" /><category term="Learning by Building" /><category term="AI Voicebot" /><category term="Toddler Persona" /><category term=".NET" /><category term="Llama3" /><category term="RAG" /><category term="Qdrant" /><category term="SymSpell" /><category term="ONNX" /><category term="LlamaGuard" /><category term="Coqui TTS" /><category term="Product Thinking" /><category term="Hands-on AI" /><category term="Raspberry Pi 5" /><category term="Edge AI" /><category term="Family Tech" /><summary type="html"><![CDATA[How a Dad, who's a Product Manager is learning AI hands-on by building a safe, voice-only LLM companion for his child - blending parenting, product thinking, and technology.]]></summary></entry><entry><title type="html">Setup Raspberry Pi 5 16GB from scratch</title><link href="https://ujwaliyer.com/posts/RaspberryPi-Setup/" rel="alternate" type="text/html" title="Setup Raspberry Pi 5 16GB from scratch" /><published>2025-11-04T00:00:00+05:30</published><updated>2025-11-04T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/RaspberryPi-Setup</id><content type="html" xml:base="https://ujwaliyer.com/posts/RaspberryPi-Setup/"><![CDATA[<h2 id="1-os-setup">1. OS Setup</h2>

<ul>
  <li>Setup Raspberry Pi OS 64-bit (Bookworm) → best for GPIO/camera + easiest device support.</li>
  <li>Download Raspberry Pi Imager from https://www.raspberrypi.com/software/</li>
  <li>Install it on your computer (Windows, macOS, or Linux)</li>
  <li>Select Raspberry Pi official OS 64-bit, the SD-card to flash, and the actual device bought</li>
  <li>Now edit settings- set hostname(raspberrypi.local), Set username(admin) and password, configure WLAN Wifi name, enable SSH</li>
  <li>Now finally write the image to the SD card- will take a while.</li>
</ul>

<p>Raspberry Pi 16GB</p>

<p><img src="/assets/img/pi-16gb.jpg" alt="" /></p>

<p>Cooler, microSD card, and 27W official charger</p>

<p><img src="/assets/img/cooler-SD-charger.png" alt="" /></p>

<h2 id="2-hardware-setup">2. Hardware Setup</h2>

<ul>
  <li>Join Cooler with the board: Align plastic connectors and stickers to the pi board and hear 2 clicks, then join fan cable to the fan connector on the board.</li>
  <li>Attach M2.HAT + NVMe SSD: Storage Solution HAT connects the NVMe drive via PCIe.</li>
  <li>Attach micro-SD card to the bottom of the pi board</li>
</ul>

<p>Pi and Cooler attached</p>

<p><img src="/assets/img/pi+cooler.png" alt="" /></p>

<p>M.2 HAT+ with 256GB SSD</p>

<p><img src="/assets/img/m2hat-case.jpg" alt="" /></p>

<p>All components combined:</p>

<p><img src="/assets/img/pi+cooler+ssd.jpg" alt="" /></p>

<h2 id="3-boot-up">3. Boot up</h2>
<ul>
  <li>Connect power and wifi network.</li>
  <li>The Pi will boot- the initial setup runs automatically.</li>
  <li>You can now SSH in from your computer: ssh username@hostname.local</li>
  <li>If you set: hostname: pi-lab, username: ujwal, You’d connect like this: ssh ujwal@pi-lab.local.
    <blockquote>
      <p>After 2 hours of debugging, this was the issue: if ur wifi or ssid is on 5ghz, you must change the channel number to 36, reboot the router, and then finally flash your SD with the image.</p>
    </blockquote>
  </li>
</ul>

<p><img src="/assets/img/wifi-fix.png" alt="" /></p>

<p>finally ssh into the pi:</p>

<p><img src="/assets/img/ssh-successful.png" alt="" /></p>

<h2 id="4-update-software">4. Update software</h2>

<p>This brings Raspberry Pi OS, firmware, and drivers up to date (especially important for Wi-Fi stability and SSD support).</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt update <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>apt full-upgrade <span class="nt">-y</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Reboot now, takes 1-2 mins.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>reboot
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now ssh back into the pi, and boot from the NVMe SSD (faster and more reliable than microSD)</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>raspi-config
</pre></td></tr></tbody></table></code></pre></div></div>
<p>Advanced Options → Boot Order → NVMe/USB Boot First
Then use the “SD Card Copier” tool (sdcardcopy) to clone your system from the microSD to the NVMe drive.
Power off, remove the microSD card, and power back on — it will now boot from the SSD.</p>

<h2 id="5-copy-sd-to-ssd-for-boot">5. Copy SD to SSD for boot</h2>

<p>Make sure nothing from the NVMe is mounted:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>umount <span class="nt">-R</span> /mnt/clone 2&gt;/dev/null <span class="o">||</span> <span class="nb">true
</span>lsblk
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Wipe leftover signatures from the NVMe:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>wipefs <span class="nt">-a</span> /dev/nvme0n1
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Clone SD → NVMe (bit-for-bit):</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo dd </span><span class="k">if</span><span class="o">=</span>/dev/mmcblk0 <span class="nv">of</span><span class="o">=</span>/dev/nvme0n1 <span class="nv">bs</span><span class="o">=</span>16M <span class="nv">status</span><span class="o">=</span>progress <span class="nv">conv</span><span class="o">=</span>fsync
<span class="nb">sudo </span>partprobe /dev/nvme0n1
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Grow the root partition to fill the NVMe. Use growpart (simplest), then grow the filesystem:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>apt-get update
<span class="nb">sudo </span>apt-get <span class="nb">install</span> <span class="nt">-y</span> cloud-guest-utils
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Expand partition 2 (root) to the end of the disk:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>growpart /dev/nvme0n1 2
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Check/resize the ext4 filesystem:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>e2fsck <span class="nt">-f</span> /dev/nvme0n1p2
<span class="nb">sudo </span>resize2fs /dev/nvme0n1p2
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Boot from NVMe:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre><span class="nb">sudo </span>poweroff
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Now remove SD card, power on the PI, it should boot from SSD. Verify after boot:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre>lsblk <span class="nt">-o</span> NAME,SIZE,FSTYPE,MOUNTPOINT
mount | <span class="nb">grep</span> <span class="s2">" / "</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p><img src="/assets/img/boot-from-ssd.PNG" alt="" /></p>

<h2 id="5-debugging-ssh-issues">5. Debugging SSH issues</h2>

<p>SSH is sometimes pretty flaky. Here are few things which worked for me.</p>

<ul>
  <li>
    <p>Ensure ssh is targetting the right ip address. Find the pi’s ip address thru your router: 
router - network - lan settings - client list. search for client name “raspberrypi”, and then ssh admin@192.168.0.112</p>
  </li>
  <li>
    <p>Check in above client list that both devices are in the same subnet should be something like this 192.168.0.1/24</p>
  </li>
  <li>
    <p>If you flashed the image manually, open the boot partition again and look for a file named:wpa_supplicant.conf. If it doesn’t exist or has wrong details, create/fix it:
```bash
country=US
ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev
update_config=1</p>
  </li>
</ul>

<p>network={
    ssid=”YourWiFiName”
    psk=”YourWiFiPassword”
}</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
7
</pre></td><td class="rouge-code"><pre>
* Router usually have 2 band wifis- 2.4ghz and 5ghz(this will have a suffix of _5G post the SSID name). Ensure the laptop ur using to ssh into the pi are in the same wifi band. 

* Clear a stale ssh host key if the Pi was reimaged/keys changed
```bash
ssh-keygen -R ip-address-of-pi
ssh-keygen -R raspberrypi.local
</pre></td></tr></tbody></table></code></pre></div></div>

<ul>
  <li>narrow down ssh issues using verbose logs
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>ssh <span class="nt">-vvv</span> admin@&lt;pi-ip&gt;
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
</ul>]]></content><author><name>Ujwal Iyer</name></author><category term="Raspberry Pi" /><category term="HAT" /><category term="NVMe SSD" /><summary type="html"><![CDATA[Setup Raspberry Pi5 16 GB, with M.2 HAT+ 256GB SSD Kit, active Cooler with heat sink- and finally install OS on the assembled Pi.]]></summary></entry><entry><title type="html">Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5</title><link href="https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot/" rel="alternate" type="text/html" title="Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5" /><published>2025-11-02T00:00:00+05:30</published><updated>2025-11-02T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot</id><content type="html" xml:base="https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot/"><![CDATA[<h2 id="vision">Vision</h2>

<ul>
  <li><strong>Hey Rama</strong> is a voice-first, completely offline learning system built on Raspberry Pi 5 (16 GB).</li>
  <li>It teaches a child topics like Ramayana, Mahabharata, Maths, Logic, Nature, Space, and Storytelling using a local Large Language Model (LLM), speech recognition, and agentic reasoning.</li>
  <li>The project helps a child explore knowledge safely, while giving a product manager like myself- a hands-on builder experience with <strong>real-world AI orchestration</strong>.</li>
</ul>

<p><img src="/assets/img/Hey-Rama.jpg" alt="" /></p>

<h2 id="feature-roadmap-and-user-stories">Feature Roadmap and User Stories</h2>

<p>Each feature is ordered for incremental build-out with clear objectives and finalized tool references.</p>

<h3 id="1-system-setup-and-environment">1. System Setup and Environment</h3>
<ul>
  <li>Objective: Prepare a stable, secure platform for offline AI workloads.</li>
  <li>Technologies: Raspberry Pi OS 64-bit, Linux, Python, Docker</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Prepare boot media, power supply, and cooling.</li>
  <li>Install Raspberry Pi OS (Bookworm 64-bit).</li>
  <li>Configure static IP, SSH, and firewall (UFW).</li>
  <li>Create <code class="language-plaintext highlighter-rouge">/data</code> directories for models, corpus, and logs.</li>
  <li>Validate system health under load.</li>
</ol>

<hr />

<h3 id="2-llm-runtime-setup">2. LLM Runtime Setup</h3>
<ul>
  <li>Objective: Enable local LLM inference through Ollama.</li>
  <li>Technologies: Ollama, GGUF 7B (Llama 3.1 or Mistral), systemd</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Install Ollama service.</li>
  <li>Pull 7B instruction-tuned model (quantized q5_k_m).</li>
  <li>Add smaller fallback model (3B) for quick responses.</li>
  <li>Verify response times &lt;8 seconds.</li>
</ol>

<hr />

<h3 id="3-knowledge-base-and-rag-pipeline">3. Knowledge Base and RAG Pipeline</h3>
<ul>
  <li>Objective: Build an offline retriever for factual and narrative content.</li>
  <li>Technologies: Qdrant, Sentence Transformers, Python</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Install Qdrant vector database.</li>
  <li>Create collections for each topic: Ramayana, Mahabharata, Maths, Logic, Stories, Nature, Space.</li>
  <li>Use local embeddings (<code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code>).</li>
  <li>Ingest chunked, tagged content into Qdrant.</li>
  <li>Validate retrieval accuracy, switch to hybrid heirarchical RAG is accuracy is low.</li>
</ol>

<hr />

<h3 id="4-voice-input-and-output">4. Voice Input and Output</h3>
<ul>
  <li>Objective: Enable hands-free, real-time interaction.</li>
  <li>Technologies: openWakeWord, Whisper.cpp, Piper</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Calibrate microphone input.</li>
  <li>Implement Whisper.cpp for offline STT.</li>
  <li>Add wake word “Hey Rama” detection.</li>
  <li>Configure Piper for offline child-friendly TTS.</li>
  <li>Test full speech loop with &lt;2s total latency.</li>
</ol>

<hr />

<h3 id="5-orchestration-and-agent-flow">5. Orchestration and Agent Flow</h3>
<ul>
  <li>Objective: Build dynamic flow between input, reasoning, and output.</li>
  <li>Technologies: LangChain (local), REST APIs, JSON flows</li>
</ul>

<p>User Stories</p>
<ol>
  <li>LangChain local orchestration.</li>
  <li>Create retrieval-augmented QA chain (RAG).</li>
  <li>Add context memory for smoother multi-turn answers.</li>
  <li>Map intents: storytelling, quiz, knowledge, logic.</li>
  <li>Validate full pipeline: wake → STT → retrieve → LLM → TTS.</li>
</ol>

<hr />

<h3 id="6-topic-specific-agents">6. Topic-Specific Agents</h3>
<ul>
  <li>Objective: Make “Hey Rama” intelligent across multiple learning domains.</li>
  <li>Technologies: LangChain, Qdrant, Ollama</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Story Agent - narrates moral and mythological stories.</li>
  <li>Quiz Agent - asks short questions and explains answers.</li>
  <li>Knowledge Agent - answers “why” and “how” questions.</li>
  <li>Each agent retrieves from its domain-specific Qdrant collection.</li>
  <li>Test tone consistency for age 5–8.</li>
</ol>

<hr />

<h3 id="7-safety-and-governance">7. Safety and Governance</h3>
<p>Objective: Keep the system factual, respectful, and child-safe.<br />
Technologies: Guardrails AI (offline), local logging</p>

<p>User Stories</p>
<ol>
  <li>Apply Guardrails filters on responses for tone and safety.</li>
  <li>Add parental control (time limits and topic access).</li>
  <li>Log all interactions for transparency.</li>
  <li>Validate safe outputs.</li>
</ol>

<hr />

<h3 id="8-monitoring-and-maintenance">8. Monitoring and Maintenance</h3>
<p>Objective: Ensure reliability and observability.<br />
Technologies: systemd, journalctl, Glances</p>

<p>User Stories:</p>
<ol>
  <li>Create systemd units for each service (Ollama, Qdrant, STT, TTS).</li>
  <li>Set up health checks and daily restarts.</li>
  <li>Log CPU/RAM usage and conversation stats.</li>
  <li>Add alerts for service failure.</li>
</ol>

<hr />

<h3 id="9-testing-and-demos">9. Testing and Demos</h3>
<p>Objective: Validate real-life usage.</p>

<p>Sample Scenarios</p>
<ol>
  <li>“Hey Rama, tell me about Hanuman.”</li>
  <li>“Quiz me on addition up to ten.”</li>
  <li>“Read a story about animals found in Africa.”</li>
  <li>“Why is the sky blue?”</li>
</ol>

<p>Each should complete end-to-end locally, with clear voice and no external connections.</p>

<hr />

<h2 id="ai--agentic-frameworks-planned-for-implementation">AI &amp; Agentic Frameworks Planned for Implementation</h2>

<table>
  <thead>
    <tr>
      <th>Framework</th>
      <th>Purpose</th>
      <th>Key Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LangChain (Local)</strong></td>
      <td>Agent orchestration between Ollama and Qdrant</td>
      <td>Chains, retrieval, reasoning</td>
    </tr>
    <tr>
      <td><strong>RAGAS</strong></td>
      <td>Evaluate retrieval accuracy and relevance</td>
      <td>RAG metrics, quality scoring</td>
    </tr>
    <tr>
      <td><strong>Whisper.cpp</strong></td>
      <td>Offline speech-to-text</td>
      <td>Real-time STT optimization</td>
    </tr>
    <tr>
      <td><strong>Piper</strong></td>
      <td>Offline text-to-speech</td>
      <td>Voice synthesis tuning</td>
    </tr>
    <tr>
      <td><strong>MemGPT</strong></td>
      <td>Add persistent memory to sessions</td>
      <td>Conversation recall, personalization</td>
    </tr>
    <tr>
      <td><strong>Guardrails AI (Offline Mode)</strong></td>
      <td>Enforce tone and safety</td>
      <td>Response filtering, schema validation</td>
    </tr>
    <tr>
      <td><strong>TruLens (Offline Eval)</strong></td>
      <td>Evaluate model helpfulness</td>
      <td>Trace and assess reasoning quality</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="learning-areas">Learning Areas</h2>

<table>
  <thead>
    <tr>
      <th>Area</th>
      <th>What to Learn?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LLM Orchestration</strong></td>
      <td>LangChain chains, context flow, structured prompts</td>
    </tr>
    <tr>
      <td><strong>Retrieval Evaluation</strong></td>
      <td>RAGAS, groundedness, recall precision</td>
    </tr>
    <tr>
      <td><strong>Conversational Memory</strong></td>
      <td>MemGPT local context persistence</td>
    </tr>
    <tr>
      <td><strong>Voice UX</strong></td>
      <td>Whisper.cpp latency tuning, Piper tone optimization</td>
    </tr>
    <tr>
      <td><strong>AI Safety</strong></td>
      <td>Guardrails rule definition and validation</td>
    </tr>
    <tr>
      <td><strong>Agent Evaluation</strong></td>
      <td>TruLens metrics and improvement loop</td>
    </tr>
    <tr>
      <td><strong>System Thinking</strong></td>
      <td>Offline-first architecture, reliability design</td>
    </tr>
  </tbody>
</table>

<hr />
<h2 id="a-product-managers-perspective">A Product Manager’s Perspective</h2>

<ul>
  <li>After over 6 yrs in product management, this project reminded me of what originally drew me to technology- the joy of <strong>building something useful from first principles</strong>.</li>
  <li>“Hey Rama” isn’t just an AI experiment; it became a full product lifecycle in miniature - discovery, design, development, validation, and iteration - all compressed into a single, tangible artifact.</li>
  <li>Every component decision - Ollama for edge inference, Qdrant for retrieval, LangChain for orchestration, Whisper and Piper for voice - mirrors the same trade-offs faced in enterprise-scale products: performance vs. usability, innovation vs. reliability, and ambition vs. maintainability.</li>
</ul>

<p>From a PM’s lens, it represents five core learnings:</p>

<ol>
  <li><strong>Start with a clear outcome</strong> - “delight the user” here meant delighting my 5-year-old curious son.</li>
  <li><strong>Design with constraints, not despite them</strong> - a 16GB Raspberry Pi forces thoughtful scoping. If you have a 4/8GB one, entire design needs to be mapped out from scratch.</li>
  <li><strong>Prioritize modularity</strong> - each addition (LangChain, MemGPT, RAGAS) had to earn its place.</li>
  <li><strong>Measure value, not volume</strong> - small, observable wins (a faster answer, a more accurate story) trump over-engineered complexity.</li>
  <li><strong>Close the feedback loop</strong> - real-world testing with a curious child is better than any synthetic evaluation metric.</li>
</ol>

<p>In essence, this project bridges <strong>AI system design</strong> and <strong>human-centered product thinking</strong>. It demonstrates that being “AI-ready” as a product manager isn’t about memorizing frameworks- it’s about <strong>understanding the why</strong> behind each layer of intelligence you introduce.</p>

<p>It’s the same muscle we use in large organizations, just exercised in a sandbox of pure creativity.</p>

<h2 id="a-fathers-perspective">A Father’s Perspective</h2>

<p>As a father, “Hey Rama” became something deeper - a bridge between my world of technology and my child’s world of imagination.</p>

<p>I am waiting for the day when my 5-year-old says <em>“Hey Rama, tell me about Hanuman”</em> and hearing a gentle, local voice respond- without screens, ads, or distractions - will sure be pretty satisfying!</p>

<p>Every late-night debugging session would be worth it, because it wasn’t just about code or AI- It was about <strong>showing my child what curiosity looks like in action</strong> - and how we can build our own tools instead of consuming someone else’s.</p>

<hr />

<p><em>© 2025 Ujwal Iyer - Reflections of a father and a PM who is learning AI by building, not by watching youtube videos and reels :-).</em></p>]]></content><author><name></name></author><category term="AI Projects" /><category term="Raspberry Pi" /><category term="LLM" /><category term="Offline AI" /><category term="Education" /><category term="Product Management" /><category term="Raspberry Pi" /><category term="Ollama" /><category term="Qdrant" /><category term="LangChain" /><category term="Whisper" /><category term="Piper" /><category term="RAG" /><category term="Edge AI" /><category term="Product Thinking" /><summary type="html"><![CDATA[A complete roadmap and offline AI framework guide to build 'Hey Rama' - a private, voice-based learning system for children using Raspberry Pi 5, Ollama, Qdrant, and LangChain.]]></summary></entry><entry><title type="html">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</title><link href="https://ujwaliyer.com/posts/AI-For-Kids/" rel="alternate" type="text/html" title="Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption" /><published>2025-10-31T00:00:00+05:30</published><updated>2025-10-31T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/AI-For-Kids</id><content type="html" xml:base="https://ujwaliyer.com/posts/AI-For-Kids/"><![CDATA[<h1 id="building-ai-for-my-5-year-old-designing-curiosity-not-consumption">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</h1>

<p>My 5-year-old is obsessed with questions.<br />
Not the “what’s two plus two” kind - but the ones that stop you mid-scroll.</p>
<blockquote>
  <p>“Why can’t we see air?” “Why is Hanuman red?” “Why does a gas flame burn blue, not orange like a matchstick?”</p>
</blockquote>

<p>Half my evenings are spent switching between ChatGPT tabs and bedtime stories. It made me wonder - if AI can answer my product strategy questions at work, why can’t it nurture his curiosity too?</p>

<p>That’s when it hit me: <em>we build AI to make adults efficient; maybe it’s time I build an AI to make kids wonder.</em></p>

<hr />

<h2 id="1-the-curiosity-persona-understanding-a-5-year-old-user">1. The “Curiosity Persona”: Understanding a 5-Year-Old User</h2>

<p>Before jumping into tools or frameworks, I did what any PM would do - built a persona.<br />
Not a B2B buyer or an enterprise admin, but a <strong>Curious Explorer aged five</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Name</strong></td>
      <td>5 yr old curious toddler</td>
    </tr>
    <tr>
      <td><strong>Motivation</strong></td>
      <td>Understand the world through stories, voices, and patterns</td>
    </tr>
    <tr>
      <td><strong>Preferred Interface</strong></td>
      <td>Voice, facial cues, sound effects, short narratives</td>
    </tr>
    <tr>
      <td><strong>Attention Span</strong></td>
      <td>2–3 minutes max per topic</td>
    </tr>
    <tr>
      <td><strong>Cognitive Model</strong></td>
      <td>Relates abstract ideas to known visuals (Hanuman = strength, fire = energy)</td>
    </tr>
    <tr>
      <td><strong>Parental Expectation</strong></td>
      <td>Safe, screen-free, emotionally aware experience</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="2-jobs-to-be-done-for-the-toddler">2. Jobs-To-Be-Done for the Toddler</h2>

<table>
  <thead>
    <tr>
      <th>Job</th>
      <th>Current Solution</th>
      <th>Pain (1–5)</th>
      <th>Gain if Solved (1–5)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Get answers to big “why” questions</td>
      <td>Parents or YouTube Kids</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Listen to Itihasa (Ramayana, Mahabharata) stories with meaning</td>
      <td>Parents reading books</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Explore science through daily life</td>
      <td>Random videos or books</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Feel heard when asking many questions</td>
      <td>Adults often tired or busy</td>
      <td>5</td>
      <td>5</td>
    </tr>
  </tbody>
</table>

<p>So the core <strong>Job to Be Done</strong> is:</p>
<blockquote>
  <p>“Help me explore my world through conversation - not content.”</p>
</blockquote>

<hr />

<h2 id="3-designing-for-zero-screen-time">3. Designing for Zero Screen Time</h2>

<p>Here’s where most AI tools fail children - they assume visual attention.<br />
But for a 5-year-old, <strong>eyes are for imagination, not interfaces</strong>.</p>

<p>So, the system must rely purely on <strong>voice + presence</strong>.</p>

<ul>
  <li><strong>Wake word model:</strong> “Hey Rama, can you tell me about rainbows?”</li>
  <li><strong>TTS engine:</strong> A friendly Indian-accented voice that blends warmth and curiosity.</li>
  <li><strong>Sound design:</strong> Each response ends with a short hum, like a “thinking” pause - helping the child know it’s listening.</li>
  <li><strong>Emotional pacing:</strong> Limit answers to 2–3 sentences. End with a “What do you think?” to spark dialogue.</li>
</ul>

<p>This isn’t a chatbot; I will try to design it like a <strong>co-explorer</strong>.</p>

<hr />

<h2 id="4-safety-architecture-how-to-make-ai-safe-for-kids">4. Safety Architecture: How to Make AI Safe for Kids</h2>

<p>Building for kids isn’t just about “PG-rated” data. It’s about emotional safety and cognitive scaffolding.</p>

<h3 id="a-guardrails">a. <strong>Guardrails</strong></h3>
<ul>
  <li>Use a <strong>local LLM (like Llama 3.1 GGUF)</strong> fine-tuned on pre-vetted content - no open internet access.</li>
  <li>Curate a <strong>knowledge corpus</strong>: Children’s encyclopedia, Amar Chitra Katha, ISRO science explainers, mythology retellings.</li>
  <li>Analyze retrievals from RAG and filter out words/meanings which are not toddler friendly</li>
</ul>

<h3 id="b-ethical-filters">b. <strong>Ethical Filters</strong></h3>
<ul>
  <li>Reinforce the phrase “I don’t know” gracefully. For example:
    <blockquote>
      <p>“That’s a deep question. Maybe we can learn that together tomorrow!”</p>
    </blockquote>
  </li>
  <li>Keep tone consistently empathetic, never corrective.</li>
</ul>

<h3 id="c-parent-mode">c. <strong>Parent Mode</strong></h3>
<ul>
  <li>Mobile dashboard for parents to view:
    <ul>
      <li>Topics explored</li>
      <li>Follow-up questions</li>
      <li>Curiosity trends (what’s peaking this week: “fire,” “planets,” or “Hanuman”)</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="5-mvp-scope-a-safe-ai-story-companion">5. MVP Scope: A Safe AI Story Companion</h2>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
      <th>PM Lens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Voice activation</td>
      <td>“Hey Mitra!” trigger using Whisper or Porcupine</td>
      <td>Accessibility</td>
    </tr>
    <tr>
      <td>Story modules</td>
      <td>Ramayana, Mahabharata, ISRO discoveries, nature sounds</td>
      <td>Engagement</td>
    </tr>
    <tr>
      <td>Question answering</td>
      <td>Short conversational responses via Llama.cpp</td>
      <td>Core value</td>
    </tr>
    <tr>
      <td>Parent dashboard</td>
      <td>Topic analytics via n8n workflow</td>
      <td>Transparency</td>
    </tr>
    <tr>
      <td>Offline mode</td>
      <td>Runs locally on Raspberry Pi</td>
      <td>Safety-first</td>
    </tr>
  </tbody>
</table>

<p><strong>North Star Metric:</strong></p>
<blockquote>
  <p>“Minutes of meaningful conversation per day (vs passive screen time).”</p>
</blockquote>

<p><strong>Guardrails:</strong></p>
<ul>
  <li>Session &lt;10 min per hour</li>
  <li>100% offline safety compliance</li>
</ul>

<p><strong>Retention goal:</strong><br />
80% weekly usage consistency by the child (measured via parent logs).</p>

<hr />

<h2 id="6-the-delight-loop">6. The Delight Loop</h2>

<p>The delight moment isn’t when it answers correctly.<br />
It’s when it <strong>asks back</strong>.</p>

<blockquote>
  <p>“Do you think Hanuman could jump because he was light like air or strong like wind?”</p>
</blockquote>

<p>That’s when a child pauses, <em>thinks</em>, and smiles. That’s engagement, not addiction.</p>

<p>That moment becomes viral - not on social media, but across living rooms. Parents talk. Builders notice. And the loop grows.</p>

<hr />

<h2 id="7-my-learning-journey-as-a-product-manager">7. My Learning Journey as a Product Manager</h2>

<p>This project isn’t just for my son - it’s a sandbox for <strong>me</strong> as a product manager.</p>

<p>I’ll be learning AI by <strong>building it hands-on</strong>, using a Raspberry Pi 5 (16GB) as the playground.<br />
Here’s what I plan to implement and learn through this:</p>

<ul>
  <li><strong>LLM integration:</strong> Running local inference with Llama 3.1 GGUF models.</li>
  <li><strong>Agentic frameworks:</strong> Building multi-agent orchestration using <strong>LangChain</strong> and <strong>LangGraph</strong>.</li>
  <li><strong>Hierarchical RAG (Retrieval-Augmented Generation):</strong> Organizing mythology, science, and nature stories in topic layers.</li>
  <li><strong>Text-to-Speech (TTS):</strong> Crafting emotionally warm, kid-friendly voice synthesis.</li>
  <li><strong>Speech-to-Text (STT):</strong> Capturing natural child speech and simplifying intent parsing.</li>
  <li><strong>Safety and moderation layer:</strong> Filtering and transforming responses for age-appropriateness.</li>
  <li><strong>Edge deployment:</strong> Running everything locally for privacy and offline reliability.</li>
  <li><strong>Voice UX testing:</strong> Observing how a child’s curiosity shapes the LLM feedback loop.</li>
</ul>

<p>So there are really <strong>three happy personas</strong> in this journey:</p>

<table>
  <thead>
    <tr>
      <th>Persona</th>
      <th>Motivation</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>The Dad</strong></td>
      <td>Sees his child delighted by learning through curiosity, not screens</td>
      <td>Pride and purpose</td>
    </tr>
    <tr>
      <td><strong>The Product Manager</strong></td>
      <td>Learns AI deeply by building, debugging, and iterating - not consuming theory</td>
      <td>Real mastery</td>
    </tr>
    <tr>
      <td><strong>The Child</strong></td>
      <td>Gets the power of an LLM through his own voice and imagination</td>
      <td>Joy and agency</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="8-product-managers-reflection">8. Product Manager’s Reflection</h2>

<p>This project taught me something no user research could - <strong>curiosity is the most underrated product metric</strong>.</p>

<p>In building this, I’m not launching another “AI for kids” app. I’m <strong>redesigning how curiosity scales safely</strong> in the age of LLMs.  If this works, I’ll have built not just a learning companion for my child - but a blueprint for how AI can grow <em>with</em> us, not <em>over</em> us.*</p>

<hr />

<h2 id="9-whats-next">9. What’s Next</h2>

<p>I’m building this project <strong>from scratch</strong> - hardware, data pipeline, orchestration, tools, and voice UX.<br />
The goal is simple:</p>
<blockquote>
  <p>To make curiosity the most natural interface between a child and AI.</p>
</blockquote>

<p>Watch this space - I’ll be sharing each milestone as I build the <strong>Kid-Safe AI Companion</strong> on Raspberry Pi 5.</p>

<p>The next post in this series will cover:  Setup of Raspberry Pi 5</p>

<hr />

<p><em>- Ujwal Iyer</em><br />
<em>Senior Product Manager | SAP Labs | Builder &amp; Dad | Learning AI by Doing</em></p>]]></content><author><name></name></author><category term="AI" /><category term="Parenting" /><category term="Product Management" /><category term="AI for kids" /><category term="LLM" /><category term="voice interface" /><category term="product management" /><category term="curiosity" /><category term="Raspberry Pi" /><category term="LangChain" /><category term="LangGraph" /><category term="RAG" /><summary type="html"><![CDATA[How a Dad, who's a Product Manager is learning AI hands-on by building a safe, voice-only LLM companion for his child - blending parenting, product thinking, and technology.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ujwaliyer.com/assets/img/ai-kid-curiosity.png" /><media:content medium="image" url="https://ujwaliyer.com/assets/img/ai-kid-curiosity.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</title><link href="https://ujwaliyer.com/posts/Ollama/" rel="alternate" type="text/html" title="Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn" /><published>2025-10-29T00:00:00+05:30</published><updated>2025-10-29T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/Ollama</id><content type="html" xml:base="https://ujwaliyer.com/posts/Ollama/"><![CDATA[<p>When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that <em>just works</em>.</p>

<p>This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution.</p>

<hr />

<h2 id="1-why-this-launch-matters">1. Why This Launch Matters</h2>

<p>Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos.<br />
Ollama changed that by shipping a <strong>self-contained LLM runtime</strong> that requires no Docker, no Python, no setup.</p>

<p>By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape.</p>

<hr />

<h2 id="2-how-ollama-works-on-windows">2. How Ollama Works on Windows</h2>

<h3 id="a-the-runtime-core---go--llamacpp"><strong>a. The Runtime Core - Go + llama.cpp</strong></h3>

<ul>
  <li>Ollama’s runtime is written in Go, not Python or Node.</li>
  <li>Go compiles into static binaries, bundling everything required to run locally - no external dependencies.</li>
  <li>Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU.</li>
</ul>

<p>This combination lets users download a single <code class="language-plaintext highlighter-rouge">.exe</code> and start running models instantly - no environment setup, no path variables, no GPU driver hunting.</p>

<p>Abstraction is adoption.
 Reducing cognitive load isn’t just good UX - it’s a growth strategy.</p>

<hr />

<h3 id="b-gpu-acceleration---directml-backend"><strong>b. GPU Acceleration - DirectML Backend</strong></h3>

<ul>
  <li>On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12.</li>
  <li>DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly.</li>
  <li>If no compatible GPU exists, Ollama falls back gracefully to CPU inference.</li>
</ul>

<p><strong>Impact:</strong><br />
Every modern Windows machine becomes a viable LLM host - even without CUDA.
PM lesson: build for the majority, not the elite hardware subset.</p>

<hr />

<h3 id="c-unified-model-format---gguf"><strong>c. Unified Model Format - GGUF</strong></h3>

<ul>
  <li>Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata.</li>
  <li>The format supports memory mapping (MMAP) - only required layers are loaded into memory.</li>
  <li>This is especially critical for Windows’ NTFS, which has higher IO overhead.</li>
</ul>

<p>Result: faster model load times, smaller memory footprint, and cleaner portability.</p>

<hr />

<h3 id="d-local-openai-compatible-api-layer"><strong>d. Local OpenAI-Compatible API Layer</strong></h3>

<p>One of Ollama’s smartest design moves lies in its API compatibility layer.<br />
It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route:</p>

<blockquote>
  <p>POST /v1/chat/completions</p>
</blockquote>

<p>This endpoint behaves identically to OpenAI’s:</p>
<ul>
  <li>Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream.</li>
  <li>Streams tokens live via Server-Sent Events (SSE).</li>
  <li>Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box.</li>
</ul>

<p>It also supports:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">/v1/completions</code> - for text-only generation</li>
  <li><code class="language-plaintext highlighter-rouge">/v1/embeddings</code> - for RAG workflows</li>
  <li><code class="language-plaintext highlighter-rouge">/api/generate</code> - Ollama’s original simple local route</li>
</ul>

<p>To switch any app from OpenAI to Ollama, simply change the base URL:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">OPENAI_API_BASE</span><span class="o">=</span><span class="s2">"http://localhost:11434/v1"</span>
<span class="nb">export </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">"ollama"</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Finally test locally:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Explain Ollama architecture"}]
  }'</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="3-setup-in-5-minutes">3. Setup in 5 Minutes</h2>

<ol>
  <li>Download Ollama for Windows → <a href="https://ollama.ai/download">ollama.ai/download</a></li>
  <li>Run the installer - it adds itself to PATH.</li>
  <li>Pull your model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>   ollama pull llama3
</pre></td></tr></tbody></table></code></pre></div></div>

<ol>
  <li>Run locally:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>ollama run llama3
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. 
Done. No Docker, no WSL, no Python virtualenvs.</li>
</ol>

<h2 id="4-key-takeaways-for-product-managers">4. Key Takeaways for Product Managers</h2>

<table>
  <thead>
    <tr>
      <th>Theme</th>
      <th>Insight</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Reduce Friction</td>
      <td>The simplest path to first success wins.</td>
      <td>One-file install over multi-step setup.</td>
    </tr>
    <tr>
      <td>Leverage Existing Standards</td>
      <td>Extend, don’t reinvent.</td>
      <td>Full <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code> parity with OpenAI.</td>
    </tr>
    <tr>
      <td>Build for the Majority</td>
      <td>Focus on mass-market hardware.</td>
      <td>DirectML for GPU abstraction across vendors.</td>
    </tr>
    <tr>
      <td>Prioritize Reliability</td>
      <td>Graceful fallbacks earn user trust.</td>
      <td>Auto-switch to CPU when GPU unavailable.</td>
    </tr>
    <tr>
      <td>Ecosystem Thinking</td>
      <td>APIs create flywheels.</td>
      <td>Works natively with LangChain, Cursor, n8n.</td>
    </tr>
    <tr>
      <td>Invisible Architecture</td>
      <td>Hide complexity behind defaults.</td>
      <td>GGUF model format with MMAP.</td>
    </tr>
    <tr>
      <td>Empathy as Strategy</td>
      <td>Developers don’t want power - they want flow.</td>
      <td>“It just works” is the true retention loop.</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Adoption isn’t driven by novelty - it’s driven by effort reduction.</p>
</blockquote>

<h2 id="5-final-thoughts">5. Final Thoughts</h2>
<p>Ollama for Windows is a rare example where technical depth meets empathetic design.
It redefined “local AI” from being an expert-only experiment to a mass-developer reality.</p>

<p>By blending:</p>
<ul>
  <li>Go’s portability</li>
  <li>DirectML’s universality</li>
  <li>GGUF’s simplicity</li>
  <li>OpenAI API compatibility</li>
</ul>

<blockquote>
  <p>Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone.</p>
</blockquote>]]></content><author><name>Ujwal Iyer</name></author><category term="AI" /><category term="LLM" /><category term="Product Management" /><category term="Ollama" /><category term="Windows" /><category term="DirectML" /><category term="Local AI" /><category term="OpenAI API" /><summary type="html"><![CDATA[A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy.]]></summary></entry><entry><title type="html">Hello World - My Journey from Developer to Architect to Product Manager in today’s age</title><link href="https://ujwaliyer.com/posts/hello-world-ujwal-iyer/" rel="alternate" type="text/html" title="Hello World - My Journey from Developer to Architect to Product Manager in today’s age" /><published>2025-10-28T00:00:00+05:30</published><updated>2025-10-28T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/hello-world-ujwal-iyer</id><content type="html" xml:base="https://ujwaliyer.com/posts/hello-world-ujwal-iyer/"><![CDATA[<h2 id="hello-there">Hello there!</h2>

<p>I’m <strong>Ujwal Iyer</strong>, a Senior Product Manager at SAP Labs, Bangalore, where I help design and scale products that reduce onboarding time and accelerate adoption.</p>

<p>Before I moved into product management, I spent years as a developer and solution architect, writing C#, building systems on Azure and AWS, and designing secure, scalable architectures for enterprise customers. That experience gave me a builder’s mindset - something I still rely on every day as a PM.</p>

<p>Today, I work at the intersection of AI, automation, and product strategy, shaping how businesses experience value from SAP’s cloud ecosystem.</p>

<hr />

<h2 id="from-developer-to-product-management">From Developer to Product Management</h2>

<ul>
  <li><strong>Developer roots</strong> - I began my career as a software engineer writing C# and .NET code, obsessed with performance, design patterns- creating trading apps, and multi-player games using Microsoft Kinect SDK and Microsoft PixelSense.</li>
  <li><strong>Architect years</strong> - Over time, I grew into a Cloud Solution Architect, designing hybrid solutions on Azure, AWS, and SAP BTP, and became a Microsoft Certified Azure Solutions Architect Expert and SAP Certified BTP Solution Architect.</li>
  <li><strong>Product management</strong> - For the last six years at SAP Labs India, I’ve led platform product with enterprise kubernetes SAP Gardener, data management solutions with SAP Data Intelligence, and at present building a greenfield SaaS platform that transforms customer onboarding from a months-long process into a guided, self-service experience.</li>
  <li><strong>Product philosophy</strong> - My focus is on blending strong technical foundations with product vision - ensuring that “how we build” never loses sight of “why we build.”</li>
</ul>

<hr />

<h2 id="what-youll-find-here">What You’ll Find Here</h2>

<p>This blog isn’t about buzzwords or corporate updates - it’s my <strong>personal lab notebook</strong>.<br />
A space to share hands-on projects, product insights, and the messy middle between technology and strategy.</p>

<p>Each post will blend <strong>architecture, PM thinking, code experiments, and AI</strong> - written for product people who like to get their hands dirty. I believe there is no greater retention in learning than by building something: from idea to outcomes.</p>

<hr />

<h2 id="why-i-started-this-blog">Why I Started This Blog</h2>

<p>We’re entering a decade where <strong>product managers can’t just “talk tech” - they need to build with it</strong>.  The ability to combine design, data, automation, platform, and architecture thinking will define the next generation of PMs.</p>

<p>This blog is my way of staying hands-on - to keep learning, to share what works (and what doesn’t), and to build a public record of experiments at the frontier of AI and product design.</p>

<hr />

<h2 id="outside-work">Outside Work</h2>

<p>When I’m not experimenting with AI workflows or writing about platforms, I’m:</p>
<ul>
  <li>building small learning projects with my 5-year-old son,</li>
  <li>reading and discussing <strong>Ramayana and Mahabharata</strong>,</li>
  <li>or helping my wife <strong>Gayathri</strong>, an urban designer, in her work on sustainable city projects across India.</li>
</ul>

<p>That balance between <strong>technology, learning, and purpose</strong> is what keeps me curious.</p>

<hr />

<h2 id="how-this-site-runs">How This Site Runs</h2>

<p>This blog is powered by <strong>Jekyll + Chirpy theme</strong>, hosted on <strong>GitHub Pages</strong>, and connected to my custom domain <a href="https://ujwaliyer.com"><strong>ujwaliyer.com</strong></a>.<br />
Everything here - from AI prototypes to product frameworks - is meant to be open, transparent, and reusable.</p>

<hr />

<p><strong>Thanks for stopping by.</strong><br />
If you’re working on something interesting or just want to swap ideas, reach me at <strong><a href="mailto:ujwaliyer@live.com">ujwaliyer@live.com</a></strong>.<br />
Let’s learn, build, and ship better products - together.</p>

<hr />

<p><em>– Ujwal Iyer</em><br />
Senior Product Manager | SAP Labs India<br />
<a href="https://ujwaliyer.com">https://ujwaliyer.com</a></p>]]></content><author><name></name></author><category term="Product Management" /><category term="AI" /><category term="Cloud" /><category term="SAP BTP" /><category term="Azure" /><category term="AWS" /><category term="Learning" /><category term="Career" /><summary type="html"><![CDATA[I’m Ujwal Iyer - Senior Product Manager at SAP Labs India, combining deep technical roots in C#, cloud architecture, and SAP BTP with a passion for building impactful products.]]></summary></entry></feed>