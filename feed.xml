<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://ujwaliyer.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://ujwaliyer.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2025-11-02T00:52:08+05:30</updated><id>https://ujwaliyer.com/feed.xml</id><title type="html">Ujwal Iyer</title><subtitle>Learnings, Experiments &amp; Notes.</subtitle><entry><title type="html">Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5</title><link href="https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot/" rel="alternate" type="text/html" title="Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5" /><published>2025-11-02T00:00:00+05:30</published><updated>2025-11-02T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot</id><content type="html" xml:base="https://ujwaliyer.com/posts/HeyRama-buildingVoiceBot/"><![CDATA[<h2 id="vision">Vision</h2>

<ul>
  <li><strong>Hey Rama</strong> is a voice-first, completely offline learning system built on Raspberry Pi 5 (16 GB).</li>
  <li>It teaches a child topics like Ramayana, Mahabharata, Maths, Logic, Nature, Space, and Storytelling using a local Large Language Model (LLM), speech recognition, and agentic reasoning.</li>
  <li>The project helps a child explore knowledge safely, while giving a product manager like myself- a hands-on builder experience with <strong>real-world AI orchestration</strong>.</li>
</ul>

<p><img src="/assets/img/Hey-Rama.jpg" alt="" /></p>

<h2 id="feature-roadmap-and-user-stories">Feature Roadmap and User Stories</h2>

<p>Each feature is ordered for incremental build-out with clear objectives and finalized tool references.</p>

<h3 id="1-system-setup-and-environment">1. System Setup and Environment</h3>
<ul>
  <li>Objective: Prepare a stable, secure platform for offline AI workloads.</li>
  <li>Technologies: Raspberry Pi OS 64-bit, Linux, Python, Docker</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Prepare boot media, power supply, and cooling.</li>
  <li>Install Raspberry Pi OS (Bookworm 64-bit).</li>
  <li>Configure static IP, SSH, and firewall (UFW).</li>
  <li>Create <code class="language-plaintext highlighter-rouge">/data</code> directories for models, corpus, and logs.</li>
  <li>Validate system health under load.</li>
</ol>

<hr />

<h3 id="2-llm-runtime-setup">2. LLM Runtime Setup</h3>
<ul>
  <li>Objective: Enable local LLM inference through Ollama.</li>
  <li>Technologies: Ollama, GGUF 7B (Llama 3.1 or Mistral), systemd</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Install Ollama service.</li>
  <li>Pull 7B instruction-tuned model (quantized q5_k_m).</li>
  <li>Add smaller fallback model (3B) for quick responses.</li>
  <li>Verify response times &lt;8 seconds.</li>
</ol>

<hr />

<h3 id="3-knowledge-base-and-rag-pipeline">3. Knowledge Base and RAG Pipeline</h3>
<ul>
  <li>Objective: Build an offline retriever for factual and narrative content.</li>
  <li>Technologies: Qdrant, Sentence Transformers, Python</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Install Qdrant vector database.</li>
  <li>Create collections for each topic: Ramayana, Mahabharata, Maths, Logic, Stories, Nature, Space.</li>
  <li>Use local embeddings (<code class="language-plaintext highlighter-rouge">all-MiniLM-L6-v2</code>).</li>
  <li>Ingest chunked, tagged content into Qdrant.</li>
  <li>Validate retrieval accuracy, switch to hybrid heirarchical RAG is accuracy is low.</li>
</ol>

<hr />

<h3 id="4-voice-input-and-output">4. Voice Input and Output</h3>
<ul>
  <li>Objective: Enable hands-free, real-time interaction.</li>
  <li>Technologies: openWakeWord, Whisper.cpp, Piper</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Calibrate microphone input.</li>
  <li>Implement Whisper.cpp for offline STT.</li>
  <li>Add wake word “Hey Rama” detection.</li>
  <li>Configure Piper for offline child-friendly TTS.</li>
  <li>Test full speech loop with &lt;2s total latency.</li>
</ol>

<hr />

<h3 id="5-orchestration-and-agent-flow">5. Orchestration and Agent Flow</h3>
<ul>
  <li>Objective: Build dynamic flow between input, reasoning, and output.</li>
  <li>Technologies: LangChain (local), REST APIs, JSON flows</li>
</ul>

<p>User Stories</p>
<ol>
  <li>LangChain local orchestration.</li>
  <li>Create retrieval-augmented QA chain (RAG).</li>
  <li>Add context memory for smoother multi-turn answers.</li>
  <li>Map intents: storytelling, quiz, knowledge, logic.</li>
  <li>Validate full pipeline: wake → STT → retrieve → LLM → TTS.</li>
</ol>

<hr />

<h3 id="6-topic-specific-agents">6. Topic-Specific Agents</h3>
<ul>
  <li>Objective: Make “Hey Rama” intelligent across multiple learning domains.</li>
  <li>Technologies: LangChain, Qdrant, Ollama</li>
</ul>

<p>User Stories</p>
<ol>
  <li>Story Agent - narrates moral and mythological stories.</li>
  <li>Quiz Agent - asks short questions and explains answers.</li>
  <li>Knowledge Agent - answers “why” and “how” questions.</li>
  <li>Each agent retrieves from its domain-specific Qdrant collection.</li>
  <li>Test tone consistency for age 5–8.</li>
</ol>

<hr />

<h3 id="7-safety-and-governance">7. Safety and Governance</h3>
<p>Objective: Keep the system factual, respectful, and child-safe.<br />
Technologies: Guardrails AI (offline), local logging</p>

<p>User Stories</p>
<ol>
  <li>Apply Guardrails filters on responses for tone and safety.</li>
  <li>Add parental control (time limits and topic access).</li>
  <li>Log all interactions for transparency.</li>
  <li>Validate safe outputs.</li>
</ol>

<hr />

<h3 id="8-monitoring-and-maintenance">8. Monitoring and Maintenance</h3>
<p>Objective: Ensure reliability and observability.<br />
Technologies: systemd, journalctl, Glances</p>

<p>User Stories:</p>
<ol>
  <li>Create systemd units for each service (Ollama, Qdrant, STT, TTS).</li>
  <li>Set up health checks and daily restarts.</li>
  <li>Log CPU/RAM usage and conversation stats.</li>
  <li>Add alerts for service failure.</li>
</ol>

<hr />

<h3 id="9-testing-and-demos">9. Testing and Demos</h3>
<p>Objective: Validate real-life usage.</p>

<p>Sample Scenarios</p>
<ol>
  <li>“Hey Rama, tell me about Hanuman.”</li>
  <li>“Quiz me on addition up to ten.”</li>
  <li>“Read a story about animals found in Africa.”</li>
  <li>“Why is the sky blue?”</li>
</ol>

<p>Each should complete end-to-end locally, with clear voice and no external connections.</p>

<hr />

<h2 id="ai--agentic-frameworks-planned-for-implementation">AI &amp; Agentic Frameworks Planned for Implementation</h2>

<table>
  <thead>
    <tr>
      <th>Framework</th>
      <th>Purpose</th>
      <th>Key Learning</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LangChain (Local)</strong></td>
      <td>Agent orchestration between Ollama and Qdrant</td>
      <td>Chains, retrieval, reasoning</td>
    </tr>
    <tr>
      <td><strong>RAGAS</strong></td>
      <td>Evaluate retrieval accuracy and relevance</td>
      <td>RAG metrics, quality scoring</td>
    </tr>
    <tr>
      <td><strong>Whisper.cpp</strong></td>
      <td>Offline speech-to-text</td>
      <td>Real-time STT optimization</td>
    </tr>
    <tr>
      <td><strong>Piper</strong></td>
      <td>Offline text-to-speech</td>
      <td>Voice synthesis tuning</td>
    </tr>
    <tr>
      <td><strong>MemGPT</strong></td>
      <td>Add persistent memory to sessions</td>
      <td>Conversation recall, personalization</td>
    </tr>
    <tr>
      <td><strong>Guardrails AI (Offline Mode)</strong></td>
      <td>Enforce tone and safety</td>
      <td>Response filtering, schema validation</td>
    </tr>
    <tr>
      <td><strong>TruLens (Offline Eval)</strong></td>
      <td>Evaluate model helpfulness</td>
      <td>Trace and assess reasoning quality</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="learning-areas">Learning Areas</h2>

<table>
  <thead>
    <tr>
      <th>Area</th>
      <th>What to Learn?</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>LLM Orchestration</strong></td>
      <td>LangChain chains, context flow, structured prompts</td>
    </tr>
    <tr>
      <td><strong>Retrieval Evaluation</strong></td>
      <td>RAGAS, groundedness, recall precision</td>
    </tr>
    <tr>
      <td><strong>Conversational Memory</strong></td>
      <td>MemGPT local context persistence</td>
    </tr>
    <tr>
      <td><strong>Voice UX</strong></td>
      <td>Whisper.cpp latency tuning, Piper tone optimization</td>
    </tr>
    <tr>
      <td><strong>AI Safety</strong></td>
      <td>Guardrails rule definition and validation</td>
    </tr>
    <tr>
      <td><strong>Agent Evaluation</strong></td>
      <td>TruLens metrics and improvement loop</td>
    </tr>
    <tr>
      <td><strong>System Thinking</strong></td>
      <td>Offline-first architecture, reliability design</td>
    </tr>
  </tbody>
</table>

<hr />
<h2 id="a-product-managers-perspective">A Product Manager’s Perspective</h2>

<ul>
  <li>After over 6 yrs in product management, this project reminded me of what originally drew me to technology- the joy of <strong>building something useful from first principles</strong>.</li>
  <li>“Hey Rama” isn’t just an AI experiment; it became a full product lifecycle in miniature - discovery, design, development, validation, and iteration - all compressed into a single, tangible artifact.</li>
  <li>Every component decision - Ollama for edge inference, Qdrant for retrieval, LangChain for orchestration, Whisper and Piper for voice - mirrors the same trade-offs faced in enterprise-scale products: performance vs. usability, innovation vs. reliability, and ambition vs. maintainability.</li>
</ul>

<p>From a PM’s lens, it represents five core learnings:</p>

<ol>
  <li><strong>Start with a clear outcome</strong> - “delight the user” here meant delighting my 5-year-old curious son.</li>
  <li><strong>Design with constraints, not despite them</strong> - a 16GB Raspberry Pi forces thoughtful scoping. If you have a 4/8GB one, entire design needs to be mapped out from scratch.</li>
  <li><strong>Prioritize modularity</strong> - each addition (LangChain, MemGPT, RAGAS) had to earn its place.</li>
  <li><strong>Measure value, not volume</strong> - small, observable wins (a faster answer, a more accurate story) trump over-engineered complexity.</li>
  <li><strong>Close the feedback loop</strong> - real-world testing with a curious child is better than any synthetic evaluation metric.</li>
</ol>

<p>In essence, this project bridges <strong>AI system design</strong> and <strong>human-centered product thinking</strong>. It demonstrates that being “AI-ready” as a product manager isn’t about memorizing frameworks- it’s about <strong>understanding the why</strong> behind each layer of intelligence you introduce.</p>

<p>It’s the same muscle we use in large organizations, just exercised in a sandbox of pure creativity.</p>

<h2 id="a-fathers-perspective">A Father’s Perspective</h2>

<p>As a father, “Hey Rama” became something deeper - a bridge between my world of technology and my child’s world of imagination.</p>

<p>I am waiting for the day when my 5-year-old says <em>“Hey Rama, tell me about Hanuman”</em> and hearing a gentle, local voice respond- without screens, ads, or distractions - will sure be pretty satisfying!</p>

<p>Every late-night debugging session would be worth it, because it wasn’t just about code or AI- It was about <strong>showing my child what curiosity looks like in action</strong> - and how we can build our own tools instead of consuming someone else’s.</p>

<hr />

<p><em>© 2025 Ujwal Iyer - Reflections of a father and a PM who is learning AI by building, not by watching youtube videos and reels :-).</em></p>]]></content><author><name></name></author><category term="AI Projects" /><category term="Raspberry Pi" /><category term="LLM" /><category term="Offline AI" /><category term="Education" /><category term="Product Management" /><category term="Raspberry Pi" /><category term="Ollama" /><category term="Qdrant" /><category term="LangChain" /><category term="Whisper" /><category term="Piper" /><category term="RAG" /><category term="Edge AI" /><category term="Product Thinking" /><summary type="html"><![CDATA[A complete roadmap and offline AI framework guide to build 'Hey Rama' - a private, voice-based learning system for children using Raspberry Pi 5, Ollama, Qdrant, and LangChain.]]></summary></entry><entry><title type="html">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</title><link href="https://ujwaliyer.com/posts/AI-For-Kids/" rel="alternate" type="text/html" title="Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption" /><published>2025-10-31T00:00:00+05:30</published><updated>2025-10-31T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/AI-For-Kids</id><content type="html" xml:base="https://ujwaliyer.com/posts/AI-For-Kids/"><![CDATA[<h1 id="building-ai-for-my-5-year-old-designing-curiosity-not-consumption">Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption</h1>

<p>My 5-year-old is obsessed with questions.<br />
Not the “what’s two plus two” kind - but the ones that stop you mid-scroll.</p>
<blockquote>
  <p>“Why can’t we see air?” “Why is Hanuman red?” “Why does a gas flame burn blue, not orange like a matchstick?”</p>
</blockquote>

<p>Half my evenings are spent switching between ChatGPT tabs and bedtime stories. It made me wonder - if AI can answer my product strategy questions at work, why can’t it nurture his curiosity too?</p>

<p>That’s when it hit me: <em>we build AI to make adults efficient; maybe it’s time I build an AI to make kids wonder.</em></p>

<hr />

<h2 id="1-the-curiosity-persona-understanding-a-5-year-old-user">1. The “Curiosity Persona”: Understanding a 5-Year-Old User</h2>

<p>Before jumping into tools or frameworks, I did what any PM would do - built a persona.<br />
Not a B2B buyer or an enterprise admin, but a <strong>Curious Explorer aged five</strong>.</p>

<table>
  <thead>
    <tr>
      <th>Attribute</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Name</strong></td>
      <td>5 yr old curious toddler</td>
    </tr>
    <tr>
      <td><strong>Motivation</strong></td>
      <td>Understand the world through stories, voices, and patterns</td>
    </tr>
    <tr>
      <td><strong>Preferred Interface</strong></td>
      <td>Voice, facial cues, sound effects, short narratives</td>
    </tr>
    <tr>
      <td><strong>Attention Span</strong></td>
      <td>2–3 minutes max per topic</td>
    </tr>
    <tr>
      <td><strong>Cognitive Model</strong></td>
      <td>Relates abstract ideas to known visuals (Hanuman = strength, fire = energy)</td>
    </tr>
    <tr>
      <td><strong>Parental Expectation</strong></td>
      <td>Safe, screen-free, emotionally aware experience</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="2-jobs-to-be-done-for-the-toddler">2. Jobs-To-Be-Done for the Toddler</h2>

<table>
  <thead>
    <tr>
      <th>Job</th>
      <th>Current Solution</th>
      <th>Pain (1–5)</th>
      <th>Gain if Solved (1–5)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Get answers to big “why” questions</td>
      <td>Parents or YouTube Kids</td>
      <td>3</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Listen to Itihasa (Ramayana, Mahabharata) stories with meaning</td>
      <td>Parents reading books</td>
      <td>2</td>
      <td>4</td>
    </tr>
    <tr>
      <td>Explore science through daily life</td>
      <td>Random videos or books</td>
      <td>4</td>
      <td>5</td>
    </tr>
    <tr>
      <td>Feel heard when asking many questions</td>
      <td>Adults often tired or busy</td>
      <td>5</td>
      <td>5</td>
    </tr>
  </tbody>
</table>

<p>So the core <strong>Job to Be Done</strong> is:</p>
<blockquote>
  <p>“Help me explore my world through conversation - not content.”</p>
</blockquote>

<hr />

<h2 id="3-designing-for-zero-screen-time">3. Designing for Zero Screen Time</h2>

<p>Here’s where most AI tools fail children - they assume visual attention.<br />
But for a 5-year-old, <strong>eyes are for imagination, not interfaces</strong>.</p>

<p>So, the system must rely purely on <strong>voice + presence</strong>.</p>

<ul>
  <li><strong>Wake word model:</strong> “Hey Rama, can you tell me about rainbows?”</li>
  <li><strong>TTS engine:</strong> A friendly Indian-accented voice that blends warmth and curiosity.</li>
  <li><strong>Sound design:</strong> Each response ends with a short hum, like a “thinking” pause - helping the child know it’s listening.</li>
  <li><strong>Emotional pacing:</strong> Limit answers to 2–3 sentences. End with a “What do you think?” to spark dialogue.</li>
</ul>

<p>This isn’t a chatbot; I will try to design it like a <strong>co-explorer</strong>.</p>

<hr />

<h2 id="4-safety-architecture-how-to-make-ai-safe-for-kids">4. Safety Architecture: How to Make AI Safe for Kids</h2>

<p>Building for kids isn’t just about “PG-rated” data. It’s about emotional safety and cognitive scaffolding.</p>

<h3 id="a-guardrails">a. <strong>Guardrails</strong></h3>
<ul>
  <li>Use a <strong>local LLM (like Llama 3.1 GGUF)</strong> fine-tuned on pre-vetted content - no open internet access.</li>
  <li>Curate a <strong>knowledge corpus</strong>: Children’s encyclopedia, Amar Chitra Katha, ISRO science explainers, mythology retellings.</li>
  <li>Analyze retrievals from RAG and filter out words/meanings which are not toddler friendly</li>
</ul>

<h3 id="b-ethical-filters">b. <strong>Ethical Filters</strong></h3>
<ul>
  <li>Reinforce the phrase “I don’t know” gracefully. For example:
    <blockquote>
      <p>“That’s a deep question. Maybe we can learn that together tomorrow!”</p>
    </blockquote>
  </li>
  <li>Keep tone consistently empathetic, never corrective.</li>
</ul>

<h3 id="c-parent-mode">c. <strong>Parent Mode</strong></h3>
<ul>
  <li>Mobile dashboard for parents to view:
    <ul>
      <li>Topics explored</li>
      <li>Follow-up questions</li>
      <li>Curiosity trends (what’s peaking this week: “fire,” “planets,” or “Hanuman”)</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="5-mvp-scope-a-safe-ai-story-companion">5. MVP Scope: A Safe AI Story Companion</h2>

<table>
  <thead>
    <tr>
      <th>Feature</th>
      <th>Description</th>
      <th>PM Lens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Voice activation</td>
      <td>“Hey Mitra!” trigger using Whisper or Porcupine</td>
      <td>Accessibility</td>
    </tr>
    <tr>
      <td>Story modules</td>
      <td>Ramayana, Mahabharata, ISRO discoveries, nature sounds</td>
      <td>Engagement</td>
    </tr>
    <tr>
      <td>Question answering</td>
      <td>Short conversational responses via Llama.cpp</td>
      <td>Core value</td>
    </tr>
    <tr>
      <td>Parent dashboard</td>
      <td>Topic analytics via n8n workflow</td>
      <td>Transparency</td>
    </tr>
    <tr>
      <td>Offline mode</td>
      <td>Runs locally on Raspberry Pi</td>
      <td>Safety-first</td>
    </tr>
  </tbody>
</table>

<p><strong>North Star Metric:</strong></p>
<blockquote>
  <p>“Minutes of meaningful conversation per day (vs passive screen time).”</p>
</blockquote>

<p><strong>Guardrails:</strong></p>
<ul>
  <li>Session &lt;10 min per hour</li>
  <li>100% offline safety compliance</li>
</ul>

<p><strong>Retention goal:</strong><br />
80% weekly usage consistency by the child (measured via parent logs).</p>

<hr />

<h2 id="6-the-delight-loop">6. The Delight Loop</h2>

<p>The delight moment isn’t when it answers correctly.<br />
It’s when it <strong>asks back</strong>.</p>

<blockquote>
  <p>“Do you think Hanuman could jump because he was light like air or strong like wind?”</p>
</blockquote>

<p>That’s when a child pauses, <em>thinks</em>, and smiles. That’s engagement, not addiction.</p>

<p>That moment becomes viral - not on social media, but across living rooms. Parents talk. Builders notice. And the loop grows.</p>

<hr />

<h2 id="7-my-learning-journey-as-a-product-manager">7. My Learning Journey as a Product Manager</h2>

<p>This project isn’t just for my son - it’s a sandbox for <strong>me</strong> as a product manager.</p>

<p>I’ll be learning AI by <strong>building it hands-on</strong>, using a Raspberry Pi 5 (16GB) as the playground.<br />
Here’s what I plan to implement and learn through this:</p>

<ul>
  <li><strong>LLM integration:</strong> Running local inference with Llama 3.1 GGUF models.</li>
  <li><strong>Agentic frameworks:</strong> Building multi-agent orchestration using <strong>LangChain</strong> and <strong>LangGraph</strong>.</li>
  <li><strong>Hierarchical RAG (Retrieval-Augmented Generation):</strong> Organizing mythology, science, and nature stories in topic layers.</li>
  <li><strong>Text-to-Speech (TTS):</strong> Crafting emotionally warm, kid-friendly voice synthesis.</li>
  <li><strong>Speech-to-Text (STT):</strong> Capturing natural child speech and simplifying intent parsing.</li>
  <li><strong>Safety and moderation layer:</strong> Filtering and transforming responses for age-appropriateness.</li>
  <li><strong>Edge deployment:</strong> Running everything locally for privacy and offline reliability.</li>
  <li><strong>Voice UX testing:</strong> Observing how a child’s curiosity shapes the LLM feedback loop.</li>
</ul>

<p>So there are really <strong>three happy personas</strong> in this journey:</p>

<table>
  <thead>
    <tr>
      <th>Persona</th>
      <th>Motivation</th>
      <th>Outcome</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>The Dad</strong></td>
      <td>Sees his child delighted by learning through curiosity, not screens</td>
      <td>Pride and purpose</td>
    </tr>
    <tr>
      <td><strong>The Product Manager</strong></td>
      <td>Learns AI deeply by building, debugging, and iterating - not consuming theory</td>
      <td>Real mastery</td>
    </tr>
    <tr>
      <td><strong>The Child</strong></td>
      <td>Gets the power of an LLM through his own voice and imagination</td>
      <td>Joy and agency</td>
    </tr>
  </tbody>
</table>

<hr />

<h2 id="8-product-managers-reflection">8. Product Manager’s Reflection</h2>

<p>This project taught me something no user research could - <strong>curiosity is the most underrated product metric</strong>.</p>

<p>In building this, I’m not launching another “AI for kids” app. I’m <strong>redesigning how curiosity scales safely</strong> in the age of LLMs.  If this works, I’ll have built not just a learning companion for my child - but a blueprint for how AI can grow <em>with</em> us, not <em>over</em> us.*</p>

<hr />

<h2 id="9-whats-next">9. What’s Next</h2>

<p>I’m building this project <strong>from scratch</strong> - hardware, data pipeline, orchestration, tools, and voice UX.<br />
The goal is simple:</p>
<blockquote>
  <p>To make curiosity the most natural interface between a child and AI.</p>
</blockquote>

<p>Watch this space - I’ll be sharing each milestone as I build the <strong>Kid-Safe AI Companion</strong> on Raspberry Pi 5.</p>

<p>The next post in this series will cover:  Setup of Raspberry Pi 5</p>

<hr />

<p><em>- Ujwal Iyer</em><br />
<em>Senior Product Manager | SAP Labs | Builder &amp; Dad | Learning AI by Doing</em></p>]]></content><author><name></name></author><category term="AI" /><category term="Parenting" /><category term="Product Management" /><category term="AI for kids" /><category term="LLM" /><category term="voice interface" /><category term="product management" /><category term="curiosity" /><category term="Raspberry Pi" /><category term="LangChain" /><category term="LangGraph" /><category term="RAG" /><summary type="html"><![CDATA[How a Dad, who's a Product Manager is learning AI hands-on by building a safe, voice-only LLM companion for his child - blending parenting, product thinking, and technology.]]></summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://ujwaliyer.com/assets/img/ai-kid-curiosity.png" /><media:content medium="image" url="https://ujwaliyer.com/assets/img/ai-kid-curiosity.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn</title><link href="https://ujwaliyer.com/posts/Ollama/" rel="alternate" type="text/html" title="Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn" /><published>2025-10-29T00:00:00+05:30</published><updated>2025-10-29T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/Ollama</id><content type="html" xml:base="https://ujwaliyer.com/posts/Ollama/"><![CDATA[<p>When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that <em>just works</em>.</p>

<p>This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution.</p>

<hr />

<h2 id="1-why-this-launch-matters">1. Why This Launch Matters</h2>

<p>Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos.<br />
Ollama changed that by shipping a <strong>self-contained LLM runtime</strong> that requires no Docker, no Python, no setup.</p>

<p>By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape.</p>

<hr />

<h2 id="2-how-ollama-works-on-windows">2. How Ollama Works on Windows</h2>

<h3 id="a-the-runtime-core---go--llamacpp"><strong>a. The Runtime Core - Go + llama.cpp</strong></h3>

<ul>
  <li>Ollama’s runtime is written in Go, not Python or Node.</li>
  <li>Go compiles into static binaries, bundling everything required to run locally - no external dependencies.</li>
  <li>Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU.</li>
</ul>

<p>This combination lets users download a single <code class="language-plaintext highlighter-rouge">.exe</code> and start running models instantly - no environment setup, no path variables, no GPU driver hunting.</p>

<p>Abstraction is adoption.
 Reducing cognitive load isn’t just good UX - it’s a growth strategy.</p>

<hr />

<h3 id="b-gpu-acceleration---directml-backend"><strong>b. GPU Acceleration - DirectML Backend</strong></h3>

<ul>
  <li>On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12.</li>
  <li>DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly.</li>
  <li>If no compatible GPU exists, Ollama falls back gracefully to CPU inference.</li>
</ul>

<p><strong>Impact:</strong><br />
Every modern Windows machine becomes a viable LLM host - even without CUDA.
PM lesson: build for the majority, not the elite hardware subset.</p>

<hr />

<h3 id="c-unified-model-format---gguf"><strong>c. Unified Model Format - GGUF</strong></h3>

<ul>
  <li>Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata.</li>
  <li>The format supports memory mapping (MMAP) - only required layers are loaded into memory.</li>
  <li>This is especially critical for Windows’ NTFS, which has higher IO overhead.</li>
</ul>

<p>Result: faster model load times, smaller memory footprint, and cleaner portability.</p>

<hr />

<h3 id="d-local-openai-compatible-api-layer"><strong>d. Local OpenAI-Compatible API Layer</strong></h3>

<p>One of Ollama’s smartest design moves lies in its API compatibility layer.<br />
It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route:</p>

<blockquote>
  <p>POST /v1/chat/completions</p>
</blockquote>

<p>This endpoint behaves identically to OpenAI’s:</p>
<ul>
  <li>Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream.</li>
  <li>Streams tokens live via Server-Sent Events (SSE).</li>
  <li>Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box.</li>
</ul>

<p>It also supports:</p>
<ul>
  <li><code class="language-plaintext highlighter-rouge">/v1/completions</code> - for text-only generation</li>
  <li><code class="language-plaintext highlighter-rouge">/v1/embeddings</code> - for RAG workflows</li>
  <li><code class="language-plaintext highlighter-rouge">/api/generate</code> - Ollama’s original simple local route</li>
</ul>

<p>To switch any app from OpenAI to Ollama, simply change the base URL:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
</pre></td><td class="rouge-code"><pre><span class="nb">export </span><span class="nv">OPENAI_API_BASE</span><span class="o">=</span><span class="s2">"http://localhost:11434/v1"</span>
<span class="nb">export </span><span class="nv">OPENAI_API_KEY</span><span class="o">=</span><span class="s2">"ollama"</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<p>Finally test locally:</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
2
3
4
5
6
</pre></td><td class="rouge-code"><pre>curl http://localhost:11434/v1/chat/completions <span class="se">\</span>
  <span class="nt">-H</span> <span class="s2">"Content-Type: application/json"</span> <span class="se">\</span>
  <span class="nt">-d</span> <span class="s1">'{
    "model": "llama3",
    "messages": [{"role": "user", "content": "Explain Ollama architecture"}]
  }'</span>
</pre></td></tr></tbody></table></code></pre></div></div>

<h2 id="3-setup-in-5-minutes">3. Setup in 5 Minutes</h2>

<ol>
  <li>Download Ollama for Windows → <a href="https://ollama.ai/download">ollama.ai/download</a></li>
  <li>Run the installer - it adds itself to PATH.</li>
  <li>Pull your model:</li>
</ol>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>   ollama pull llama3
</pre></td></tr></tbody></table></code></pre></div></div>

<ol>
  <li>Run locally:
    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><table class="rouge-table"><tbody><tr><td class="rouge-gutter gl"><pre class="lineno">1
</pre></td><td class="rouge-code"><pre>ollama run llama3
</pre></td></tr></tbody></table></code></pre></div>    </div>
  </li>
  <li>Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. 
Done. No Docker, no WSL, no Python virtualenvs.</li>
</ol>

<h2 id="4-key-takeaways-for-product-managers">4. Key Takeaways for Product Managers</h2>

<table>
  <thead>
    <tr>
      <th>Theme</th>
      <th>Insight</th>
      <th>Example</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Reduce Friction</td>
      <td>The simplest path to first success wins.</td>
      <td>One-file install over multi-step setup.</td>
    </tr>
    <tr>
      <td>Leverage Existing Standards</td>
      <td>Extend, don’t reinvent.</td>
      <td>Full <code class="language-plaintext highlighter-rouge">/v1/chat/completions</code> parity with OpenAI.</td>
    </tr>
    <tr>
      <td>Build for the Majority</td>
      <td>Focus on mass-market hardware.</td>
      <td>DirectML for GPU abstraction across vendors.</td>
    </tr>
    <tr>
      <td>Prioritize Reliability</td>
      <td>Graceful fallbacks earn user trust.</td>
      <td>Auto-switch to CPU when GPU unavailable.</td>
    </tr>
    <tr>
      <td>Ecosystem Thinking</td>
      <td>APIs create flywheels.</td>
      <td>Works natively with LangChain, Cursor, n8n.</td>
    </tr>
    <tr>
      <td>Invisible Architecture</td>
      <td>Hide complexity behind defaults.</td>
      <td>GGUF model format with MMAP.</td>
    </tr>
    <tr>
      <td>Empathy as Strategy</td>
      <td>Developers don’t want power - they want flow.</td>
      <td>“It just works” is the true retention loop.</td>
    </tr>
  </tbody>
</table>

<blockquote>
  <p>Adoption isn’t driven by novelty - it’s driven by effort reduction.</p>
</blockquote>

<h2 id="5-final-thoughts">5. Final Thoughts</h2>
<p>Ollama for Windows is a rare example where technical depth meets empathetic design.
It redefined “local AI” from being an expert-only experiment to a mass-developer reality.</p>

<p>By blending:</p>
<ul>
  <li>Go’s portability</li>
  <li>DirectML’s universality</li>
  <li>GGUF’s simplicity</li>
  <li>OpenAI API compatibility</li>
</ul>

<blockquote>
  <p>Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone.</p>
</blockquote>]]></content><author><name>Ujwal Iyer</name></author><category term="AI" /><category term="LLM" /><category term="Product Management" /><category term="Ollama" /><category term="Windows" /><category term="DirectML" /><category term="Local AI" /><category term="OpenAI API" /><summary type="html"><![CDATA[A deep teardown of Ollama for Windows - how it made local LLMs simple, what powers it under the hood, and what product managers can learn from its design and strategy.]]></summary></entry><entry><title type="html">Hello World - My Journey from Developer to Architect to Product Manager in today’s age</title><link href="https://ujwaliyer.com/posts/hello-world-ujwal-iyer/" rel="alternate" type="text/html" title="Hello World - My Journey from Developer to Architect to Product Manager in today’s age" /><published>2025-10-28T00:00:00+05:30</published><updated>2025-10-28T00:00:00+05:30</updated><id>https://ujwaliyer.com/posts/hello-world-ujwal-iyer</id><content type="html" xml:base="https://ujwaliyer.com/posts/hello-world-ujwal-iyer/"><![CDATA[<h2 id="hello-there">Hello there!</h2>

<p>I’m <strong>Ujwal Iyer</strong>, a Senior Product Manager at SAP Labs, Bangalore, where I help design and scale products that reduce onboarding time and accelerate adoption.</p>

<p>Before I moved into product management, I spent years as a developer and solution architect, writing C#, building systems on Azure and AWS, and designing secure, scalable architectures for enterprise customers. That experience gave me a builder’s mindset - something I still rely on every day as a PM.</p>

<p>Today, I work at the intersection of AI, automation, and product strategy, shaping how businesses experience value from SAP’s cloud ecosystem.</p>

<hr />

<h2 id="from-developer-to-product-management">From Developer to Product Management</h2>

<ul>
  <li><strong>Developer roots</strong> - I began my career as a software engineer writing C# and .NET code, obsessed with performance, design patterns- creating trading apps, and multi-player games using Microsoft Kinect SDK and Microsoft PixelSense.</li>
  <li><strong>Architect years</strong> - Over time, I grew into a Cloud Solution Architect, designing hybrid solutions on Azure, AWS, and SAP BTP, and became a Microsoft Certified Azure Solutions Architect Expert and SAP Certified BTP Solution Architect.</li>
  <li><strong>Product management</strong> - For the last six years at SAP Labs India, I’ve led platform product with enterprise kubernetes SAP Gardener, data management solutions with SAP Data Intelligence, and at present building a greenfield SaaS platform that transforms customer onboarding from a months-long process into a guided, self-service experience.</li>
  <li><strong>Product philosophy</strong> - My focus is on blending strong technical foundations with product vision - ensuring that “how we build” never loses sight of “why we build.”</li>
</ul>

<hr />

<h2 id="what-youll-find-here">What You’ll Find Here</h2>

<p>This blog isn’t about buzzwords or corporate updates - it’s my <strong>personal lab notebook</strong>.<br />
A space to share hands-on projects, product insights, and the messy middle between technology and strategy.</p>

<p>Each post will blend <strong>architecture, PM thinking, code experiments, and AI</strong> - written for product people who like to get their hands dirty. I believe there is no greater retention in learning than by building something: from idea to outcomes.</p>

<hr />

<h2 id="why-i-started-this-blog">Why I Started This Blog</h2>

<p>We’re entering a decade where <strong>product managers can’t just “talk tech” - they need to build with it</strong>.  The ability to combine design, data, automation, platform, and architecture thinking will define the next generation of PMs.</p>

<p>This blog is my way of staying hands-on - to keep learning, to share what works (and what doesn’t), and to build a public record of experiments at the frontier of AI and product design.</p>

<hr />

<h2 id="outside-work">Outside Work</h2>

<p>When I’m not experimenting with AI workflows or writing about platforms, I’m:</p>
<ul>
  <li>building small learning projects with my 5-year-old son,</li>
  <li>reading and discussing <strong>Ramayana and Mahabharata</strong>,</li>
  <li>or helping my wife <strong>Gayathri</strong>, an urban designer, in her work on sustainable city projects across India.</li>
</ul>

<p>That balance between <strong>technology, learning, and purpose</strong> is what keeps me curious.</p>

<hr />

<h2 id="how-this-site-runs">How This Site Runs</h2>

<p>This blog is powered by <strong>Jekyll + Chirpy theme</strong>, hosted on <strong>GitHub Pages</strong>, and connected to my custom domain <a href="https://ujwaliyer.com"><strong>ujwaliyer.com</strong></a>.<br />
Everything here - from AI prototypes to product frameworks - is meant to be open, transparent, and reusable.</p>

<hr />

<p><strong>Thanks for stopping by.</strong><br />
If you’re working on something interesting or just want to swap ideas, reach me at <strong><a href="mailto:ujwaliyer@live.com">ujwaliyer@live.com</a></strong>.<br />
Let’s learn, build, and ship better products - together.</p>

<hr />

<p><em>– Ujwal Iyer</em><br />
Senior Product Manager | SAP Labs India<br />
<a href="https://ujwaliyer.com">https://ujwaliyer.com</a></p>]]></content><author><name></name></author><category term="Product Management" /><category term="AI" /><category term="Cloud" /><category term="SAP BTP" /><category term="Azure" /><category term="AWS" /><category term="Learning" /><category term="Career" /><summary type="html"><![CDATA[I’m Ujwal Iyer - Senior Product Manager at SAP Labs India, combining deep technical roots in C#, cloud architecture, and SAP BTP with a passion for building impactful products.]]></summary></entry></feed>