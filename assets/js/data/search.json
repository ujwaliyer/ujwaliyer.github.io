[ { "title": "Unpacking toddler persona & AI-ML solutions for an AI Voice Bot", "url": "/posts/ToddlerPersonaResearch-AI-Solutions/", "categories": "AI Projects, Product Management, Raspberry Pi, Learning by Building", "tags": "AI Voicebot, Toddler Persona, .NET, Llama3, RAG, Qdrant, SymSpell, ONNX, LlamaGuard, Coqui TTS, Product Thinking, Hands-on AI, Raspberry Pi 5, Edge AI, Family Tech", "date": "2025-11-06 00:00:00 +0530", "content": "As a Senior Product Manager, I’ve spent years obsessing over user personas, friction points, and success metrics. But nothing prepared me for my newest and toughest customer: my five-year-old toddler. He doesn’t just ask questions. He fires rapid, chaotic queries at bedtime. “Appa, why Rama go to forest?” (where to start- because his Dasharatha told him so, or because he promised Kaikeyi that he will grant her boon when the time came?) “Why did Krishna help Arjuna but not Duryodhana?” (tough to explain righteousness/dharma) “Why gas fire blue but diya fire red?” (I still don’t know!) This blog will focus on pure user research, and suggest AI frameworks and subsequent solutions of the pain point faced by me. 1. Persona Deep Dive: “The Curious toddler” Who he is Age: 5 years Context: bilingual (English + Tamil) Hardware: Raspberry Pi 5, 16GB Interface: Voice only - no screen, no keyboard Behavior Traits Speaks in short, broken english Has a 20-second patience window hence easily distracted - responses longer than 20 seconds are lost Emotionally reactive - cannot have wrong tone or scary words 2. Jobs-to-be-Done + Pain/Gain Matrix Job Current Workaround Pain (1–5) Gain if Solved (1–5) Understand epic stories in his own words Parents explain manually 4 5 Ask science “why” questions safely YouTube Kids or Khan Academy 3 5 Stay engaged for short bursts Cartoons 5 4 Speak naturally and be understood Speech-to-text confusion 4 5 3. Friction Heatmap Stage Friction Severity (1–5) Fix Priority Voice to Text Broken grammar 5 High Query Understanding Needs semantic rewriting 5 High Answer Generation Too long / too abstract 4 High Response Filtering Unsafe/adult/violent content 5 Critical Text-to-Speech Must sound friendly, excited, high-pitch, and child-like 3 Medium 4. Success Metrics Attention Retention: responses under 20 seconds Understanding Score: answers correctly paraphrased by child Safe Response Ratio: age-safe answers Latency: TTS response delivered under 2 seconds 5. Technical Blueprint: AI Concepts to solve Friction &amp; Jobs to be done Here’s the system I designed for now: Step Who Handles It Why Speech-to-Text Cleanup SymSpell Fast spelling and grammar correction Prompt Rewriting Llama 3.1 7B (Quantized) Reformulates prompts so a chunked vector matches correctly Vectorize Content ONNX MiniLM Transformer based embeddings Storage/Retrieval Qdrant Scalable vector DB and similarity search Answer Generation Llama 3.1 7B again Generates short, story-like responses Kiddy Guardrails LlamaGuard Filters complex or violent content before TTS Text-to-Speech (TTS) Coqui TTS or System.Speech Produces warm, human-like voice output Each component runs locally on the Raspberry Pi, respecting privacy and keeping everything offline. No cloud calls, no hidden data drift - just pure, safe, curious AI for a curious child. 6. Why .NET Makes It Possible I chose .NET not just out of nostalgia, but also because the AI ecosystem quietly supports all of this: STT + TTS → System.Speech namespace or Coqui integration via C# Qdrant Vector DB → Official .NET client with REST bindings ONNX Runtime → Runs MiniLM embeddings natively on CPU Llama 3.1 7B (GGUF) → Compatible via llama.cpp bindings for .NET SymSpell.NET → Lightweight typo and grammar fixer LLamaSharp → C# binding for llama.cpp so can load LlamaGuard model Watch this space I’ll be sharing the step-by-step build of this AI Toddler Explorer soon - from RAG pipelines to voice tuning - all on a humble Raspberry Pi 5. Follow my journey at ujwaliyer.com - where bedtime questions meet machine learning." }, { "title": "Setup Raspberry Pi 5 16GB from scratch", "url": "/posts/RaspberryPi-Setup/", "categories": "", "tags": "Raspberry Pi, HAT, NVMe SSD", "date": "2025-11-04 00:00:00 +0530", "content": "1. OS Setup Setup Raspberry Pi OS 64-bit (Bookworm) → best for GPIO/camera + easiest device support. Download Raspberry Pi Imager from https://www.raspberrypi.com/software/ Install it on your computer (Windows, macOS, or Linux) Select Raspberry Pi official OS 64-bit, the SD-card to flash, and the actual device bought Now edit settings- set hostname(raspberrypi.local), Set username(admin) and password, configure WLAN Wifi name, enable SSH Now finally write the image to the SD card- will take a while. Raspberry Pi 16GB Cooler, microSD card, and 27W official charger 2. Hardware Setup Join Cooler with the board: Align plastic connectors and stickers to the pi board and hear 2 clicks, then join fan cable to the fan connector on the board. Attach M2.HAT + NVMe SSD: Storage Solution HAT connects the NVMe drive via PCIe. Attach micro-SD card to the bottom of the pi board Pi and Cooler attached M.2 HAT+ with 256GB SSD All components combined: 3. Boot up Connect power and wifi network. The Pi will boot- the initial setup runs automatically. You can now SSH in from your computer: ssh username@hostname.local If you set: hostname: pi-lab, username: ujwal, You’d connect like this: ssh ujwal@pi-lab.local. After 2 hours of debugging, this was the issue: if ur wifi or ssid is on 5ghz, you must change the channel number to 36, reboot the router, and then finally flash your SD with the image. finally ssh into the pi: 4. Update software This brings Raspberry Pi OS, firmware, and drivers up to date (especially important for Wi-Fi stability and SSD support). sudo apt update &amp;&amp; sudo apt full-upgrade -y Reboot now, takes 1-2 mins. sudo reboot Now ssh back into the pi, and boot from the NVMe SSD (faster and more reliable than microSD) sudo raspi-config Advanced Options → Boot Order → NVMe/USB Boot First Then use the “SD Card Copier” tool (sdcardcopy) to clone your system from the microSD to the NVMe drive. Power off, remove the microSD card, and power back on — it will now boot from the SSD. 5. Copy SD to SSD for boot Make sure nothing from the NVMe is mounted: sudo umount -R /mnt/clone 2&gt;/dev/null || true lsblk Wipe leftover signatures from the NVMe: sudo wipefs -a /dev/nvme0n1 Clone SD → NVMe (bit-for-bit): sudo dd if=/dev/mmcblk0 of=/dev/nvme0n1 bs=16M status=progress conv=fsync sudo partprobe /dev/nvme0n1 Grow the root partition to fill the NVMe. Use growpart (simplest), then grow the filesystem: sudo apt-get update sudo apt-get install -y cloud-guest-utils Expand partition 2 (root) to the end of the disk: sudo growpart /dev/nvme0n1 2 Check/resize the ext4 filesystem: sudo e2fsck -f /dev/nvme0n1p2 sudo resize2fs /dev/nvme0n1p2 Boot from NVMe: sudo poweroff Now remove SD card, power on the PI, it should boot from SSD. Verify after boot: lsblk -o NAME,SIZE,FSTYPE,MOUNTPOINT mount | grep \" / \" 5. Debugging SSH issues SSH is sometimes pretty flaky. Here are few things which worked for me. Ensure ssh is targetting the right ip address. Find the pi’s ip address thru your router: router - network - lan settings - client list. search for client name “raspberrypi”, and then ssh admin@192.168.0.112 Check in above client list that both devices are in the same subnet should be something like this 192.168.0.1/24 If you flashed the image manually, open the boot partition again and look for a file named:wpa_supplicant.conf. If it doesn’t exist or has wrong details, create/fix it: ```bash country=US ctrl_interface=DIR=/var/run/wpa_supplicant GROUP=netdev update_config=1 network={ ssid=”YourWiFiName” psk=”YourWiFiPassword” } * Router usually have 2 band wifis- 2.4ghz and 5ghz(this will have a suffix of _5G post the SSID name). Ensure the laptop ur using to ssh into the pi are in the same wifi band. * Clear a stale ssh host key if the Pi was reimaged/keys changed ```bash ssh-keygen -R ip-address-of-pi ssh-keygen -R raspberrypi.local narrow down ssh issues using verbose logs ssh -vvv admin@&lt;pi-ip&gt;" }, { "title": "Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5", "url": "/posts/HeyRama-buildingVoiceBot/", "categories": "AI Projects, Raspberry Pi, LLM, Offline AI, Education, Product Management", "tags": "Raspberry Pi, Ollama, Qdrant, LangChain, Whisper, Piper, RAG, Edge AI, Product Thinking", "date": "2025-11-02 00:00:00 +0530", "content": "Vision Hey Rama is a voice-first, completely offline learning system built on Raspberry Pi 5 (16 GB). It teaches a child topics like Ramayana, Mahabharata, Maths, Logic, Nature, Space, and Storytelling using a local Large Language Model (LLM), speech recognition, and agentic reasoning. The project helps a child explore knowledge safely, while giving a product manager like myself- a hands-on builder experience with real-world AI orchestration. Feature Roadmap and User Stories Each feature is ordered for incremental build-out with clear objectives and finalized tool references. 1. System Setup and Environment Objective: Prepare a stable, secure platform for offline AI workloads. Technologies: Raspberry Pi OS 64-bit, Linux, Python, Docker User Stories Prepare boot media, power supply, and cooling. Install Raspberry Pi OS (Bookworm 64-bit). Configure static IP, SSH, and firewall (UFW). Create /data directories for models, corpus, and logs. Validate system health under load. 2. LLM Runtime Setup Objective: Enable local LLM inference through Ollama. Technologies: Ollama, GGUF 7B (Llama 3.1 or Mistral), systemd User Stories Install Ollama service. Pull 7B instruction-tuned model (quantized q5_k_m). Add smaller fallback model (3B) for quick responses. Verify response times &lt;8 seconds. 3. Knowledge Base and RAG Pipeline Objective: Build an offline retriever for factual and narrative content. Technologies: Qdrant, Sentence Transformers, Python User Stories Install Qdrant vector database. Create collections for each topic: Ramayana, Mahabharata, Maths, Logic, Stories, Nature, Space. Use local embeddings (all-MiniLM-L6-v2). Ingest chunked, tagged content into Qdrant. Validate retrieval accuracy, switch to hybrid heirarchical RAG is accuracy is low. 4. Voice Input and Output Objective: Enable hands-free, real-time interaction. Technologies: openWakeWord, Whisper.cpp, Piper User Stories Calibrate microphone input. Implement Whisper.cpp for offline STT. Add wake word “Hey Rama” detection. Configure Piper for offline child-friendly TTS. Test full speech loop with &lt;2s total latency. 5. Orchestration and Agent Flow Objective: Build dynamic flow between input, reasoning, and output. Technologies: LangChain (local), REST APIs, JSON flows User Stories LangChain local orchestration. Create retrieval-augmented QA chain (RAG). Add context memory for smoother multi-turn answers. Map intents: storytelling, quiz, knowledge, logic. Validate full pipeline: wake → STT → retrieve → LLM → TTS. 6. Topic-Specific Agents Objective: Make “Hey Rama” intelligent across multiple learning domains. Technologies: LangChain, Qdrant, Ollama User Stories Story Agent - narrates moral and mythological stories. Quiz Agent - asks short questions and explains answers. Knowledge Agent - answers “why” and “how” questions. Each agent retrieves from its domain-specific Qdrant collection. Test tone consistency for age 5–8. 7. Safety and Governance Objective: Keep the system factual, respectful, and child-safe. Technologies: Guardrails AI (offline), local logging User Stories Apply Guardrails filters on responses for tone and safety. Add parental control (time limits and topic access). Log all interactions for transparency. Validate safe outputs. 8. Monitoring and Maintenance Objective: Ensure reliability and observability. Technologies: systemd, journalctl, Glances User Stories: Create systemd units for each service (Ollama, Qdrant, STT, TTS). Set up health checks and daily restarts. Log CPU/RAM usage and conversation stats. Add alerts for service failure. 9. Testing and Demos Objective: Validate real-life usage. Sample Scenarios “Hey Rama, tell me about Hanuman.” “Quiz me on addition up to ten.” “Read a story about animals found in Africa.” “Why is the sky blue?” Each should complete end-to-end locally, with clear voice and no external connections. AI &amp; Agentic Frameworks Planned for Implementation Framework Purpose Key Learning LangChain (Local) Agent orchestration between Ollama and Qdrant Chains, retrieval, reasoning RAGAS Evaluate retrieval accuracy and relevance RAG metrics, quality scoring Whisper.cpp Offline speech-to-text Real-time STT optimization Piper Offline text-to-speech Voice synthesis tuning MemGPT Add persistent memory to sessions Conversation recall, personalization Guardrails AI (Offline Mode) Enforce tone and safety Response filtering, schema validation TruLens (Offline Eval) Evaluate model helpfulness Trace and assess reasoning quality Learning Areas Area What to Learn? LLM Orchestration LangChain chains, context flow, structured prompts Retrieval Evaluation RAGAS, groundedness, recall precision Conversational Memory MemGPT local context persistence Voice UX Whisper.cpp latency tuning, Piper tone optimization AI Safety Guardrails rule definition and validation Agent Evaluation TruLens metrics and improvement loop System Thinking Offline-first architecture, reliability design A Product Manager’s Perspective After over 6 yrs in product management, this project reminded me of what originally drew me to technology- the joy of building something useful from first principles. “Hey Rama” isn’t just an AI experiment; it became a full product lifecycle in miniature - discovery, design, development, validation, and iteration - all compressed into a single, tangible artifact. Every component decision - Ollama for edge inference, Qdrant for retrieval, LangChain for orchestration, Whisper and Piper for voice - mirrors the same trade-offs faced in enterprise-scale products: performance vs. usability, innovation vs. reliability, and ambition vs. maintainability. From a PM’s lens, it represents five core learnings: Start with a clear outcome - “delight the user” here meant delighting my 5-year-old curious son. Design with constraints, not despite them - a 16GB Raspberry Pi forces thoughtful scoping. If you have a 4/8GB one, entire design needs to be mapped out from scratch. Prioritize modularity - each addition (LangChain, MemGPT, RAGAS) had to earn its place. Measure value, not volume - small, observable wins (a faster answer, a more accurate story) trump over-engineered complexity. Close the feedback loop - real-world testing with a curious child is better than any synthetic evaluation metric. In essence, this project bridges AI system design and human-centered product thinking. It demonstrates that being “AI-ready” as a product manager isn’t about memorizing frameworks- it’s about understanding the why behind each layer of intelligence you introduce. It’s the same muscle we use in large organizations, just exercised in a sandbox of pure creativity. A Father’s Perspective As a father, “Hey Rama” became something deeper - a bridge between my world of technology and my child’s world of imagination. I am waiting for the day when my 5-year-old says “Hey Rama, tell me about Hanuman” and hearing a gentle, local voice respond- without screens, ads, or distractions - will sure be pretty satisfying! Every late-night debugging session would be worth it, because it wasn’t just about code or AI- It was about showing my child what curiosity looks like in action - and how we can build our own tools instead of consuming someone else’s. © 2025 Ujwal Iyer - Reflections of a father and a PM who is learning AI by building, not by watching youtube videos and reels :-)." }, { "title": "Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption", "url": "/posts/AI-For-Kids/", "categories": "AI, Parenting, Product Management", "tags": "AI for kids, LLM, voice interface, product management, curiosity, Raspberry Pi, LangChain, LangGraph, RAG", "date": "2025-10-31 00:00:00 +0530", "content": "Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption My 5-year-old is obsessed with questions. Not the “what’s two plus two” kind - but the ones that stop you mid-scroll. “Why can’t we see air?” “Why is Hanuman red?” “Why does a gas flame burn blue, not orange like a matchstick?” Half my evenings are spent switching between ChatGPT tabs and bedtime stories. It made me wonder - if AI can answer my product strategy questions at work, why can’t it nurture his curiosity too? That’s when it hit me: we build AI to make adults efficient; maybe it’s time I build an AI to make kids wonder. 1. The “Curiosity Persona”: Understanding a 5-Year-Old User Before jumping into tools or frameworks, I did what any PM would do - built a persona. Not a B2B buyer or an enterprise admin, but a Curious Explorer aged five. Attribute Description Name 5 yr old curious toddler Motivation Understand the world through stories, voices, and patterns Preferred Interface Voice, facial cues, sound effects, short narratives Attention Span 2–3 minutes max per topic Cognitive Model Relates abstract ideas to known visuals (Hanuman = strength, fire = energy) Parental Expectation Safe, screen-free, emotionally aware experience 2. Jobs-To-Be-Done for the Toddler Job Current Solution Pain (1–5) Gain if Solved (1–5) Get answers to big “why” questions Parents or YouTube Kids 3 5 Listen to Itihasa (Ramayana, Mahabharata) stories with meaning Parents reading books 2 4 Explore science through daily life Random videos or books 4 5 Feel heard when asking many questions Adults often tired or busy 5 5 So the core Job to Be Done is: “Help me explore my world through conversation - not content.” 3. Designing for Zero Screen Time Here’s where most AI tools fail children - they assume visual attention. But for a 5-year-old, eyes are for imagination, not interfaces. So, the system must rely purely on voice + presence. Wake word model: “Hey Rama, can you tell me about rainbows?” TTS engine: A friendly Indian-accented voice that blends warmth and curiosity. Sound design: Each response ends with a short hum, like a “thinking” pause - helping the child know it’s listening. Emotional pacing: Limit answers to 2–3 sentences. End with a “What do you think?” to spark dialogue. This isn’t a chatbot; I will try to design it like a co-explorer. 4. Safety Architecture: How to Make AI Safe for Kids Building for kids isn’t just about “PG-rated” data. It’s about emotional safety and cognitive scaffolding. a. Guardrails Use a local LLM (like Llama 3.1 GGUF) fine-tuned on pre-vetted content - no open internet access. Curate a knowledge corpus: Children’s encyclopedia, Amar Chitra Katha, ISRO science explainers, mythology retellings. Analyze retrievals from RAG and filter out words/meanings which are not toddler friendly b. Ethical Filters Reinforce the phrase “I don’t know” gracefully. For example: “That’s a deep question. Maybe we can learn that together tomorrow!” Keep tone consistently empathetic, never corrective. c. Parent Mode Mobile dashboard for parents to view: Topics explored Follow-up questions Curiosity trends (what’s peaking this week: “fire,” “planets,” or “Hanuman”) 5. MVP Scope: A Safe AI Story Companion Feature Description PM Lens Voice activation “Hey Mitra!” trigger using Whisper or Porcupine Accessibility Story modules Ramayana, Mahabharata, ISRO discoveries, nature sounds Engagement Question answering Short conversational responses via Llama.cpp Core value Parent dashboard Topic analytics via n8n workflow Transparency Offline mode Runs locally on Raspberry Pi Safety-first North Star Metric: “Minutes of meaningful conversation per day (vs passive screen time).” Guardrails: Session &lt;10 min per hour 100% offline safety compliance Retention goal: 80% weekly usage consistency by the child (measured via parent logs). 6. The Delight Loop The delight moment isn’t when it answers correctly. It’s when it asks back. “Do you think Hanuman could jump because he was light like air or strong like wind?” That’s when a child pauses, thinks, and smiles. That’s engagement, not addiction. That moment becomes viral - not on social media, but across living rooms. Parents talk. Builders notice. And the loop grows. 7. My Learning Journey as a Product Manager This project isn’t just for my son - it’s a sandbox for me as a product manager. I’ll be learning AI by building it hands-on, using a Raspberry Pi 5 (16GB) as the playground. Here’s what I plan to implement and learn through this: LLM integration: Running local inference with Llama 3.1 GGUF models. Agentic frameworks: Building multi-agent orchestration using LangChain and LangGraph. Hierarchical RAG (Retrieval-Augmented Generation): Organizing mythology, science, and nature stories in topic layers. Text-to-Speech (TTS): Crafting emotionally warm, kid-friendly voice synthesis. Speech-to-Text (STT): Capturing natural child speech and simplifying intent parsing. Safety and moderation layer: Filtering and transforming responses for age-appropriateness. Edge deployment: Running everything locally for privacy and offline reliability. Voice UX testing: Observing how a child’s curiosity shapes the LLM feedback loop. So there are really three happy personas in this journey: Persona Motivation Outcome The Dad Sees his child delighted by learning through curiosity, not screens Pride and purpose The Product Manager Learns AI deeply by building, debugging, and iterating - not consuming theory Real mastery The Child Gets the power of an LLM through his own voice and imagination Joy and agency 8. Product Manager’s Reflection This project taught me something no user research could - curiosity is the most underrated product metric. In building this, I’m not launching another “AI for kids” app. I’m redesigning how curiosity scales safely in the age of LLMs. If this works, I’ll have built not just a learning companion for my child - but a blueprint for how AI can grow with us, not over us.* 9. What’s Next I’m building this project from scratch - hardware, data pipeline, orchestration, tools, and voice UX. The goal is simple: To make curiosity the most natural interface between a child and AI. Watch this space - I’ll be sharing each milestone as I build the Kid-Safe AI Companion on Raspberry Pi 5. The next post in this series will cover: Setup of Raspberry Pi 5 - Ujwal Iyer Senior Product Manager | SAP Labs | Builder &amp; Dad | Learning AI by Doing" }, { "title": "Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn", "url": "/posts/Ollama/", "categories": "", "tags": "AI, LLM, Product Management, Ollama, Windows, DirectML, Local AI, OpenAI API", "date": "2025-10-29 00:00:00 +0530", "content": "When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that just works. This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution. 1. Why This Launch Matters Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos. Ollama changed that by shipping a self-contained LLM runtime that requires no Docker, no Python, no setup. By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape. 2. How Ollama Works on Windows a. The Runtime Core - Go + llama.cpp Ollama’s runtime is written in Go, not Python or Node. Go compiles into static binaries, bundling everything required to run locally - no external dependencies. Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU. This combination lets users download a single .exe and start running models instantly - no environment setup, no path variables, no GPU driver hunting. Abstraction is adoption. Reducing cognitive load isn’t just good UX - it’s a growth strategy. b. GPU Acceleration - DirectML Backend On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12. DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly. If no compatible GPU exists, Ollama falls back gracefully to CPU inference. Impact: Every modern Windows machine becomes a viable LLM host - even without CUDA. PM lesson: build for the majority, not the elite hardware subset. c. Unified Model Format - GGUF Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata. The format supports memory mapping (MMAP) - only required layers are loaded into memory. This is especially critical for Windows’ NTFS, which has higher IO overhead. Result: faster model load times, smaller memory footprint, and cleaner portability. d. Local OpenAI-Compatible API Layer One of Ollama’s smartest design moves lies in its API compatibility layer. It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route: POST /v1/chat/completions This endpoint behaves identically to OpenAI’s: Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream. Streams tokens live via Server-Sent Events (SSE). Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box. It also supports: /v1/completions - for text-only generation /v1/embeddings - for RAG workflows /api/generate - Ollama’s original simple local route To switch any app from OpenAI to Ollama, simply change the base URL: export OPENAI_API_BASE=\"http://localhost:11434/v1\" export OPENAI_API_KEY=\"ollama\" Finally test locally: curl http://localhost:11434/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"llama3\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain Ollama architecture\"}] }' 3. Setup in 5 Minutes Download Ollama for Windows → ollama.ai/download Run the installer - it adds itself to PATH. Pull your model: ollama pull llama3 Run locally: ollama run llama3 Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. Done. No Docker, no WSL, no Python virtualenvs. 4. Key Takeaways for Product Managers Theme Insight Example Reduce Friction The simplest path to first success wins. One-file install over multi-step setup. Leverage Existing Standards Extend, don’t reinvent. Full /v1/chat/completions parity with OpenAI. Build for the Majority Focus on mass-market hardware. DirectML for GPU abstraction across vendors. Prioritize Reliability Graceful fallbacks earn user trust. Auto-switch to CPU when GPU unavailable. Ecosystem Thinking APIs create flywheels. Works natively with LangChain, Cursor, n8n. Invisible Architecture Hide complexity behind defaults. GGUF model format with MMAP. Empathy as Strategy Developers don’t want power - they want flow. “It just works” is the true retention loop. Adoption isn’t driven by novelty - it’s driven by effort reduction. 5. Final Thoughts Ollama for Windows is a rare example where technical depth meets empathetic design. It redefined “local AI” from being an expert-only experiment to a mass-developer reality. By blending: Go’s portability DirectML’s universality GGUF’s simplicity OpenAI API compatibility Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone." }, { "title": "Hello World - My Journey from Developer to Architect to Product Manager in today's age", "url": "/posts/hello-world-ujwal-iyer/", "categories": "", "tags": "Product Management, AI, Cloud, SAP BTP, Azure, AWS, Learning, Career", "date": "2025-10-28 00:00:00 +0530", "content": "Hello there! I’m Ujwal Iyer, a Senior Product Manager at SAP Labs, Bangalore, where I help design and scale products that reduce onboarding time and accelerate adoption. Before I moved into product management, I spent years as a developer and solution architect, writing C#, building systems on Azure and AWS, and designing secure, scalable architectures for enterprise customers. That experience gave me a builder’s mindset - something I still rely on every day as a PM. Today, I work at the intersection of AI, automation, and product strategy, shaping how businesses experience value from SAP’s cloud ecosystem. From Developer to Product Management Developer roots - I began my career as a software engineer writing C# and .NET code, obsessed with performance, design patterns- creating trading apps, and multi-player games using Microsoft Kinect SDK and Microsoft PixelSense. Architect years - Over time, I grew into a Cloud Solution Architect, designing hybrid solutions on Azure, AWS, and SAP BTP, and became a Microsoft Certified Azure Solutions Architect Expert and SAP Certified BTP Solution Architect. Product management - For the last six years at SAP Labs India, I’ve led platform product with enterprise kubernetes SAP Gardener, data management solutions with SAP Data Intelligence, and at present building a greenfield SaaS platform that transforms customer onboarding from a months-long process into a guided, self-service experience. Product philosophy - My focus is on blending strong technical foundations with product vision - ensuring that “how we build” never loses sight of “why we build.” What You’ll Find Here This blog isn’t about buzzwords or corporate updates - it’s my personal lab notebook. A space to share hands-on projects, product insights, and the messy middle between technology and strategy. Each post will blend architecture, PM thinking, code experiments, and AI - written for product people who like to get their hands dirty. I believe there is no greater retention in learning than by building something: from idea to outcomes. Why I Started This Blog We’re entering a decade where product managers can’t just “talk tech” - they need to build with it. The ability to combine design, data, automation, platform, and architecture thinking will define the next generation of PMs. This blog is my way of staying hands-on - to keep learning, to share what works (and what doesn’t), and to build a public record of experiments at the frontier of AI and product design. Outside Work When I’m not experimenting with AI workflows or writing about platforms, I’m: building small learning projects with my 5-year-old son, reading and discussing Ramayana and Mahabharata, or helping my wife Gayathri, an urban designer, in her work on sustainable city projects across India. That balance between technology, learning, and purpose is what keeps me curious. How This Site Runs This blog is powered by Jekyll + Chirpy theme, hosted on GitHub Pages, and connected to my custom domain ujwaliyer.com. Everything here - from AI prototypes to product frameworks - is meant to be open, transparent, and reusable. Thanks for stopping by. If you’re working on something interesting or just want to swap ideas, reach me at ujwaliyer@live.com. Let’s learn, build, and ship better products - together. – Ujwal Iyer Senior Product Manager | SAP Labs India https://ujwaliyer.com" } ]
