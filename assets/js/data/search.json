[ { "title": "Hey Rama: Building a Voice-First Offline Learning Companion on Raspberry Pi 5", "url": "/posts/HeyRama-buildingVoiceBot/", "categories": "AI Projects, Raspberry Pi, LLM, Offline AI, Education, Product Management", "tags": "Raspberry Pi, Ollama, Qdrant, LangChain, Whisper, Piper, RAG, Edge AI, Product Thinking", "date": "2025-11-02 00:00:00 +0530", "content": "Vision Hey Rama is a voice-first, completely offline learning system built on Raspberry Pi 5 (16 GB). It teaches a child topics like Ramayana, Mahabharata, Maths, Logic, Nature, Space, and Storytelling using a local Large Language Model (LLM), speech recognition, and agentic reasoning. The project helps a child explore knowledge safely, while giving a product manager like myself- a hands-on builder experience with real-world AI orchestration. Feature Roadmap and User Stories Each feature is ordered for incremental build-out with clear objectives and finalized tool references. 1. System Setup and Environment Objective: Prepare a stable, secure platform for offline AI workloads. Technologies: Raspberry Pi OS 64-bit, Linux, Python, Docker User Stories Prepare boot media, power supply, and cooling. Install Raspberry Pi OS (Bookworm 64-bit). Configure static IP, SSH, and firewall (UFW). Create /data directories for models, corpus, and logs. Validate system health under load. 2. LLM Runtime Setup Objective: Enable local LLM inference through Ollama. Technologies: Ollama, GGUF 7B (Llama 3.1 or Mistral), systemd User Stories Install Ollama service. Pull 7B instruction-tuned model (quantized q5_k_m). Add smaller fallback model (3B) for quick responses. Verify response times &lt;8 seconds. 3. Knowledge Base and RAG Pipeline Objective: Build an offline retriever for factual and narrative content. Technologies: Qdrant, Sentence Transformers, Python User Stories Install Qdrant vector database. Create collections for each topic: Ramayana, Mahabharata, Maths, Logic, Stories, Nature, Space. Use local embeddings (all-MiniLM-L6-v2). Ingest chunked, tagged content into Qdrant. Validate retrieval accuracy, switch to hybrid heirarchical RAG is accuracy is low. 4. Voice Input and Output Objective: Enable hands-free, real-time interaction. Technologies: openWakeWord, Whisper.cpp, Piper User Stories Calibrate microphone input. Implement Whisper.cpp for offline STT. Add wake word “Hey Rama” detection. Configure Piper for offline child-friendly TTS. Test full speech loop with &lt;2s total latency. 5. Orchestration and Agent Flow Objective: Build dynamic flow between input, reasoning, and output. Technologies: LangChain (local), REST APIs, JSON flows User Stories LangChain local orchestration. Create retrieval-augmented QA chain (RAG). Add context memory for smoother multi-turn answers. Map intents: storytelling, quiz, knowledge, logic. Validate full pipeline: wake → STT → retrieve → LLM → TTS. 6. Topic-Specific Agents Objective: Make “Hey Rama” intelligent across multiple learning domains. Technologies: LangChain, Qdrant, Ollama User Stories Story Agent - narrates moral and mythological stories. Quiz Agent - asks short questions and explains answers. Knowledge Agent - answers “why” and “how” questions. Each agent retrieves from its domain-specific Qdrant collection. Test tone consistency for age 5–8. 7. Safety and Governance Objective: Keep the system factual, respectful, and child-safe. Technologies: Guardrails AI (offline), local logging User Stories Apply Guardrails filters on responses for tone and safety. Add parental control (time limits and topic access). Log all interactions for transparency. Validate safe outputs. 8. Monitoring and Maintenance Objective: Ensure reliability and observability. Technologies: systemd, journalctl, Glances User Stories: Create systemd units for each service (Ollama, Qdrant, STT, TTS). Set up health checks and daily restarts. Log CPU/RAM usage and conversation stats. Add alerts for service failure. 9. Testing and Demos Objective: Validate real-life usage. Sample Scenarios “Hey Rama, tell me about Hanuman.” “Quiz me on addition up to ten.” “Read a story about animals found in Africa.” “Why is the sky blue?” Each should complete end-to-end locally, with clear voice and no external connections. AI &amp; Agentic Frameworks Planned for Implementation Framework Purpose Key Learning LangChain (Local) Agent orchestration between Ollama and Qdrant Chains, retrieval, reasoning RAGAS Evaluate retrieval accuracy and relevance RAG metrics, quality scoring Whisper.cpp Offline speech-to-text Real-time STT optimization Piper Offline text-to-speech Voice synthesis tuning MemGPT Add persistent memory to sessions Conversation recall, personalization Guardrails AI (Offline Mode) Enforce tone and safety Response filtering, schema validation TruLens (Offline Eval) Evaluate model helpfulness Trace and assess reasoning quality Learning Areas Area What to Learn? LLM Orchestration LangChain chains, context flow, structured prompts Retrieval Evaluation RAGAS, groundedness, recall precision Conversational Memory MemGPT local context persistence Voice UX Whisper.cpp latency tuning, Piper tone optimization AI Safety Guardrails rule definition and validation Agent Evaluation TruLens metrics and improvement loop System Thinking Offline-first architecture, reliability design A Product Manager’s Perspective After over 6 yrs in product management, this project reminded me of what originally drew me to technology- the joy of building something useful from first principles. “Hey Rama” isn’t just an AI experiment; it became a full product lifecycle in miniature - discovery, design, development, validation, and iteration - all compressed into a single, tangible artifact. Every component decision - Ollama for edge inference, Qdrant for retrieval, LangChain for orchestration, Whisper and Piper for voice - mirrors the same trade-offs faced in enterprise-scale products: performance vs. usability, innovation vs. reliability, and ambition vs. maintainability. From a PM’s lens, it represents five core learnings: Start with a clear outcome - “delight the user” here meant delighting my 5-year-old curious son. Design with constraints, not despite them - a 16GB Raspberry Pi forces thoughtful scoping. If you have a 4/8GB one, entire design needs to be mapped out from scratch. Prioritize modularity - each addition (LangChain, MemGPT, RAGAS) had to earn its place. Measure value, not volume - small, observable wins (a faster answer, a more accurate story) trump over-engineered complexity. Close the feedback loop - real-world testing with a curious child is better than any synthetic evaluation metric. In essence, this project bridges AI system design and human-centered product thinking. It demonstrates that being “AI-ready” as a product manager isn’t about memorizing frameworks- it’s about understanding the why behind each layer of intelligence you introduce. It’s the same muscle we use in large organizations, just exercised in a sandbox of pure creativity. A Father’s Perspective As a father, “Hey Rama” became something deeper - a bridge between my world of technology and my child’s world of imagination. I am waiting for the day when my 5-year-old says “Hey Rama, tell me about Hanuman” and hearing a gentle, local voice respond- without screens, ads, or distractions - will sure be pretty satisfying! Every late-night debugging session would be worth it, because it wasn’t just about code or AI- It was about showing my child what curiosity looks like in action - and how we can build our own tools instead of consuming someone else’s. © 2025 Ujwal Iyer - Reflections of a father and a PM who is learning AI by building, not by watching youtube videos and reels :-)." }, { "title": "Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption", "url": "/posts/AI-For-Kids/", "categories": "AI, Parenting, Product Management", "tags": "AI for kids, LLM, voice interface, product management, curiosity, Raspberry Pi, LangChain, LangGraph, RAG", "date": "2025-10-31 00:00:00 +0530", "content": "Building AI for My 5-Year-Old: Designing Curiosity, Not Consumption My 5-year-old is obsessed with questions. Not the “what’s two plus two” kind - but the ones that stop you mid-scroll. “Why can’t we see air?” “Why is Hanuman red?” “Why does a gas flame burn blue, not orange like a matchstick?” Half my evenings are spent switching between ChatGPT tabs and bedtime stories. It made me wonder - if AI can answer my product strategy questions at work, why can’t it nurture his curiosity too? That’s when it hit me: we build AI to make adults efficient; maybe it’s time I build an AI to make kids wonder. 1. The “Curiosity Persona”: Understanding a 5-Year-Old User Before jumping into tools or frameworks, I did what any PM would do - built a persona. Not a B2B buyer or an enterprise admin, but a Curious Explorer aged five. Attribute Description Name 5 yr old curious toddler Motivation Understand the world through stories, voices, and patterns Preferred Interface Voice, facial cues, sound effects, short narratives Attention Span 2–3 minutes max per topic Cognitive Model Relates abstract ideas to known visuals (Hanuman = strength, fire = energy) Parental Expectation Safe, screen-free, emotionally aware experience 2. Jobs-To-Be-Done for the Toddler Job Current Solution Pain (1–5) Gain if Solved (1–5) Get answers to big “why” questions Parents or YouTube Kids 3 5 Listen to Itihasa (Ramayana, Mahabharata) stories with meaning Parents reading books 2 4 Explore science through daily life Random videos or books 4 5 Feel heard when asking many questions Adults often tired or busy 5 5 So the core Job to Be Done is: “Help me explore my world through conversation - not content.” 3. Designing for Zero Screen Time Here’s where most AI tools fail children - they assume visual attention. But for a 5-year-old, eyes are for imagination, not interfaces. So, the system must rely purely on voice + presence. Wake word model: “Hey Rama, can you tell me about rainbows?” TTS engine: A friendly Indian-accented voice that blends warmth and curiosity. Sound design: Each response ends with a short hum, like a “thinking” pause - helping the child know it’s listening. Emotional pacing: Limit answers to 2–3 sentences. End with a “What do you think?” to spark dialogue. This isn’t a chatbot; I will try to design it like a co-explorer. 4. Safety Architecture: How to Make AI Safe for Kids Building for kids isn’t just about “PG-rated” data. It’s about emotional safety and cognitive scaffolding. a. Guardrails Use a local LLM (like Llama 3.1 GGUF) fine-tuned on pre-vetted content - no open internet access. Curate a knowledge corpus: Children’s encyclopedia, Amar Chitra Katha, ISRO science explainers, mythology retellings. Analyze retrievals from RAG and filter out words/meanings which are not toddler friendly b. Ethical Filters Reinforce the phrase “I don’t know” gracefully. For example: “That’s a deep question. Maybe we can learn that together tomorrow!” Keep tone consistently empathetic, never corrective. c. Parent Mode Mobile dashboard for parents to view: Topics explored Follow-up questions Curiosity trends (what’s peaking this week: “fire,” “planets,” or “Hanuman”) 5. MVP Scope: A Safe AI Story Companion Feature Description PM Lens Voice activation “Hey Mitra!” trigger using Whisper or Porcupine Accessibility Story modules Ramayana, Mahabharata, ISRO discoveries, nature sounds Engagement Question answering Short conversational responses via Llama.cpp Core value Parent dashboard Topic analytics via n8n workflow Transparency Offline mode Runs locally on Raspberry Pi Safety-first North Star Metric: “Minutes of meaningful conversation per day (vs passive screen time).” Guardrails: Session &lt;10 min per hour 100% offline safety compliance Retention goal: 80% weekly usage consistency by the child (measured via parent logs). 6. The Delight Loop The delight moment isn’t when it answers correctly. It’s when it asks back. “Do you think Hanuman could jump because he was light like air or strong like wind?” That’s when a child pauses, thinks, and smiles. That’s engagement, not addiction. That moment becomes viral - not on social media, but across living rooms. Parents talk. Builders notice. And the loop grows. 7. My Learning Journey as a Product Manager This project isn’t just for my son - it’s a sandbox for me as a product manager. I’ll be learning AI by building it hands-on, using a Raspberry Pi 5 (16GB) as the playground. Here’s what I plan to implement and learn through this: LLM integration: Running local inference with Llama 3.1 GGUF models. Agentic frameworks: Building multi-agent orchestration using LangChain and LangGraph. Hierarchical RAG (Retrieval-Augmented Generation): Organizing mythology, science, and nature stories in topic layers. Text-to-Speech (TTS): Crafting emotionally warm, kid-friendly voice synthesis. Speech-to-Text (STT): Capturing natural child speech and simplifying intent parsing. Safety and moderation layer: Filtering and transforming responses for age-appropriateness. Edge deployment: Running everything locally for privacy and offline reliability. Voice UX testing: Observing how a child’s curiosity shapes the LLM feedback loop. So there are really three happy personas in this journey: Persona Motivation Outcome The Dad Sees his child delighted by learning through curiosity, not screens Pride and purpose The Product Manager Learns AI deeply by building, debugging, and iterating - not consuming theory Real mastery The Child Gets the power of an LLM through his own voice and imagination Joy and agency 8. Product Manager’s Reflection This project taught me something no user research could - curiosity is the most underrated product metric. In building this, I’m not launching another “AI for kids” app. I’m redesigning how curiosity scales safely in the age of LLMs. If this works, I’ll have built not just a learning companion for my child - but a blueprint for how AI can grow with us, not over us.* 9. What’s Next I’m building this project from scratch - hardware, data pipeline, orchestration, tools, and voice UX. The goal is simple: To make curiosity the most natural interface between a child and AI. Watch this space - I’ll be sharing each milestone as I build the Kid-Safe AI Companion on Raspberry Pi 5. The next post in this series will cover: Setup of Raspberry Pi 5 - Ujwal Iyer Senior Product Manager | SAP Labs | Builder &amp; Dad | Learning AI by Doing" }, { "title": "Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn", "url": "/posts/Ollama/", "categories": "", "tags": "AI, LLM, Product Management, Ollama, Windows, DirectML, Local AI, OpenAI API", "date": "2025-10-29 00:00:00 +0530", "content": "When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that just works. This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution. 1. Why This Launch Matters Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos. Ollama changed that by shipping a self-contained LLM runtime that requires no Docker, no Python, no setup. By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape. 2. How Ollama Works on Windows a. The Runtime Core - Go + llama.cpp Ollama’s runtime is written in Go, not Python or Node. Go compiles into static binaries, bundling everything required to run locally - no external dependencies. Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU. This combination lets users download a single .exe and start running models instantly - no environment setup, no path variables, no GPU driver hunting. Abstraction is adoption. Reducing cognitive load isn’t just good UX - it’s a growth strategy. b. GPU Acceleration - DirectML Backend On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12. DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly. If no compatible GPU exists, Ollama falls back gracefully to CPU inference. Impact: Every modern Windows machine becomes a viable LLM host - even without CUDA. PM lesson: build for the majority, not the elite hardware subset. c. Unified Model Format - GGUF Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata. The format supports memory mapping (MMAP) - only required layers are loaded into memory. This is especially critical for Windows’ NTFS, which has higher IO overhead. Result: faster model load times, smaller memory footprint, and cleaner portability. d. Local OpenAI-Compatible API Layer One of Ollama’s smartest design moves lies in its API compatibility layer. It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route: POST /v1/chat/completions This endpoint behaves identically to OpenAI’s: Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream. Streams tokens live via Server-Sent Events (SSE). Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box. It also supports: /v1/completions - for text-only generation /v1/embeddings - for RAG workflows /api/generate - Ollama’s original simple local route To switch any app from OpenAI to Ollama, simply change the base URL: export OPENAI_API_BASE=\"http://localhost:11434/v1\" export OPENAI_API_KEY=\"ollama\" Finally test locally: curl http://localhost:11434/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"llama3\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain Ollama architecture\"}] }' 3. Setup in 5 Minutes Download Ollama for Windows → ollama.ai/download Run the installer - it adds itself to PATH. Pull your model: ollama pull llama3 Run locally: ollama run llama3 Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. Done. No Docker, no WSL, no Python virtualenvs. 4. Key Takeaways for Product Managers Theme Insight Example Reduce Friction The simplest path to first success wins. One-file install over multi-step setup. Leverage Existing Standards Extend, don’t reinvent. Full /v1/chat/completions parity with OpenAI. Build for the Majority Focus on mass-market hardware. DirectML for GPU abstraction across vendors. Prioritize Reliability Graceful fallbacks earn user trust. Auto-switch to CPU when GPU unavailable. Ecosystem Thinking APIs create flywheels. Works natively with LangChain, Cursor, n8n. Invisible Architecture Hide complexity behind defaults. GGUF model format with MMAP. Empathy as Strategy Developers don’t want power - they want flow. “It just works” is the true retention loop. Adoption isn’t driven by novelty - it’s driven by effort reduction. 5. Final Thoughts Ollama for Windows is a rare example where technical depth meets empathetic design. It redefined “local AI” from being an expert-only experiment to a mass-developer reality. By blending: Go’s portability DirectML’s universality GGUF’s simplicity OpenAI API compatibility Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone." }, { "title": "Hello World - My Journey from Developer to Architect to Product Manager in today's age", "url": "/posts/hello-world-ujwal-iyer/", "categories": "", "tags": "Product Management, AI, Cloud, SAP BTP, Azure, AWS, Learning, Career", "date": "2025-10-28 00:00:00 +0530", "content": "Hello there! I’m Ujwal Iyer, a Senior Product Manager at SAP Labs, Bangalore, where I help design and scale products that reduce onboarding time and accelerate adoption. Before I moved into product management, I spent years as a developer and solution architect, writing C#, building systems on Azure and AWS, and designing secure, scalable architectures for enterprise customers. That experience gave me a builder’s mindset - something I still rely on every day as a PM. Today, I work at the intersection of AI, automation, and product strategy, shaping how businesses experience value from SAP’s cloud ecosystem. From Developer to Product Management Developer roots - I began my career as a software engineer writing C# and .NET code, obsessed with performance, design patterns- creating trading apps, and multi-player games using Microsoft Kinect SDK and Microsoft PixelSense. Architect years - Over time, I grew into a Cloud Solution Architect, designing hybrid solutions on Azure, AWS, and SAP BTP, and became a Microsoft Certified Azure Solutions Architect Expert and SAP Certified BTP Solution Architect. Product management - For the last six years at SAP Labs India, I’ve led platform product with enterprise kubernetes SAP Gardener, data management solutions with SAP Data Intelligence, and at present building a greenfield SaaS platform that transforms customer onboarding from a months-long process into a guided, self-service experience. Product philosophy - My focus is on blending strong technical foundations with product vision - ensuring that “how we build” never loses sight of “why we build.” What You’ll Find Here This blog isn’t about buzzwords or corporate updates - it’s my personal lab notebook. A space to share hands-on projects, product insights, and the messy middle between technology and strategy. Each post will blend architecture, PM thinking, code experiments, and AI - written for product people who like to get their hands dirty. I believe there is no greater retention in learning than by building something: from idea to outcomes. Why I Started This Blog We’re entering a decade where product managers can’t just “talk tech” - they need to build with it. The ability to combine design, data, automation, platform, and architecture thinking will define the next generation of PMs. This blog is my way of staying hands-on - to keep learning, to share what works (and what doesn’t), and to build a public record of experiments at the frontier of AI and product design. Outside Work When I’m not experimenting with AI workflows or writing about platforms, I’m: building small learning projects with my 5-year-old son, reading and discussing Ramayana and Mahabharata, or helping my wife Gayathri, an urban designer, in her work on sustainable city projects across India. That balance between technology, learning, and purpose is what keeps me curious. How This Site Runs This blog is powered by Jekyll + Chirpy theme, hosted on GitHub Pages, and connected to my custom domain ujwaliyer.com. Everything here - from AI prototypes to product frameworks - is meant to be open, transparent, and reusable. Thanks for stopping by. If you’re working on something interesting or just want to swap ideas, reach me at ujwaliyer@live.com. Let’s learn, build, and ship better products - together. – Ujwal Iyer Senior Product Manager | SAP Labs India https://ujwaliyer.com" } ]
