[ { "title": "Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn", "url": "/posts/Ollama/", "categories": "", "tags": "AI, LLM, Product Management, Ollama, Windows, DirectML, Local AI, OpenAI API", "date": "2025-10-29 00:00:00 +0530", "content": "Ollama for Windows: How Local LLMs Finally Work - And What Product Managers Can Learn When Ollama shipped native support for Windows in 2025, it wasn’t just another port. It was the product equivalent of a breakthrough - transforming the complex world of open-weight LLMs into a single command-line experience that just works. This product teardown explores how Ollama for Windows works, the specific technical innovations that made it possible, and what product managers can take away from its execution. 1. Why This Launch Matters Until recently, running a model like Llama 3 or Mistral locally on Windows meant long hours of driver installs, CUDA mismatches, and dependency chaos. Ollama changed that by shipping a self-contained LLM runtime that requires no Docker, no Python, no setup. By extending that simplicity to Windows - still the world’s dominant OS for developers and enterprises - Ollama unlocked a massive new user base and, in doing so, quietly shifted the local AI landscape. 2. How Ollama Works on Windows a. The Runtime Core - Go + llama.cpp Ollama’s runtime is written in Go, not Python or Node. Go compiles into static binaries, bundling everything required to run locally - no external dependencies. Under the hood, Ollama uses llama.cpp, a highly optimized C++ engine that performs quantized inference directly on CPU or GPU. This combination lets users download a single .exe and start running models instantly - no environment setup, no path variables, no GPU driver hunting. Abstraction is adoption. Reducing cognitive load isn’t just good UX - it’s a growth strategy. b. GPU Acceleration - DirectML Backend On Windows, Ollama uses DirectML, Microsoft’s hardware-agnostic ML acceleration layer built on DirectX 12. DirectML automatically detects available GPUs - NVIDIA, AMD, or Intel - and routes compute workloads accordingly. If no compatible GPU exists, Ollama falls back gracefully to CPU inference. Impact: Every modern Windows machine becomes a viable LLM host - even without CUDA. PM lesson: build for the majority, not the elite hardware subset. c. Unified Model Format - GGUF Ollama standardized on GGUF, a binary format that bundles model weights, tokenizer data, and quantization metadata. The format supports memory mapping (MMAP) - only required layers are loaded into memory. This is especially critical for Windows’ NTFS, which has higher IO overhead. Result: faster model load times, smaller memory footprint, and cleaner portability. d. Local OpenAI-Compatible API Layer One of Ollama’s smartest design moves lies in its API compatibility layer. It exposes a local HTTP server at http://localhost:11434 that mirrors the OpenAI REST API, including the most widely used route: POST /v1/chat/completions This endpoint behaves identically to OpenAI’s: Accepts model, messages[] (roles: system, user, assistant), and standard parameters like temperature, max_tokens, and stream. Streams tokens live via Server-Sent Events (SSE). Returns OpenAI-compatible JSON responses - meaning tools like LangChain, LlamaIndex, Cursor, and n8n work out-of-the-box. It also supports: /v1/completions - for text-only generation /v1/embeddings - for RAG workflows /api/generate - Ollama’s original simple local route To switch any app from OpenAI to Ollama, simply change the base URL: export OPENAI_API_BASE=\"http://localhost:11434/v1\" export OPENAI_API_KEY=\"ollama\" Finally test locally: curl http://localhost:11434/v1/chat/completions \\ -H \"Content-Type: application/json\" \\ -d '{ \"model\": \"llama3\", \"messages\": [{\"role\": \"user\", \"content\": \"Explain Ollama architecture\"}] }' 3. Setup in 5 Minutes Download Ollama for Windows → ollama.ai/download Run the installer - it adds itself to PATH. Pull your model: ollama pull llama3 Run locally: ollama run llama3 Integrate via API: Point your OpenAI-compatible tool to http://localhost:11434/v1. Done. No Docker, no WSL, no Python virtualenvs. 4. Key Takeaways for Product Managers Theme Insight Example Reduce Friction The simplest path to first success wins. One-file install over multi-step setup. Leverage Existing Standards Extend, don’t reinvent. Full /v1/chat/completions parity with OpenAI. Build for the Majority Focus on mass-market hardware. DirectML for GPU abstraction across vendors. Prioritize Reliability Graceful fallbacks earn user trust. Auto-switch to CPU when GPU unavailable. Ecosystem Thinking APIs create flywheels. Works natively with LangChain, Cursor, n8n. Invisible Architecture Hide complexity behind defaults. GGUF model format with MMAP. Empathy as Strategy Developers don’t want power - they want flow. “It just works” is the true retention loop. Adoption isn’t driven by novelty - it’s driven by effort reduction. 5. Final Thoughts Ollama for Windows is a rare example where technical depth meets empathetic design. It redefined “local AI” from being an expert-only experiment to a mass-developer reality. By blending: Go’s portability DirectML’s universality GGUF’s simplicity OpenAI API compatibility Ollama didn’t just ship a Windows binary - it shipped a platform thesis: local, private, interoperable AI for everyone." }, { "title": "Hello World - My Journey from Developer to Architect to Product Manager in today's age", "url": "/posts/hello-world-ujwal-iyer/", "categories": "", "tags": "Product Management, AI, Cloud, SAP BTP, Azure, AWS, Learning, Career", "date": "2025-10-28 00:00:00 +0530", "content": "Hello there! I’m Ujwal Iyer, a Senior Product Manager at SAP Labs, Bangalore, where I help design and scale products that reduce onboarding time and accelerate adoption. Before I moved into product management, I spent years as a developer and solution architect, writing C#, building systems on Azure and AWS, and designing secure, scalable architectures for enterprise customers. That experience gave me a builder’s mindset - something I still rely on every day as a PM. Today, I work at the intersection of AI, automation, and product strategy, shaping how businesses experience value from SAP’s cloud ecosystem. From Developer to Product Management Developer roots - I began my career as a software engineer writing C# and .NET code, obsessed with performance, design patterns- creating trading apps, and multi-player games using Microsoft Kinect SDK and Microsoft PixelSense. Architect years - Over time, I grew into a Cloud Solution Architect, designing hybrid solutions on Azure, AWS, and SAP BTP, and became a Microsoft Certified Azure Solutions Architect Expert and SAP Certified BTP Solution Architect. Product management - For the last six years at SAP Labs India, I’ve led platform product with enterprise kubernetes SAP Gardener, data management solutions with SAP Data Intelligence, and at present building a greenfield SaaS platform that transforms customer onboarding from a months-long process into a guided, self-service experience. Product philosophy - My focus is on blending strong technical foundations with product vision - ensuring that “how we build” never loses sight of “why we build.” What You’ll Find Here This blog isn’t about buzzwords or corporate updates - it’s my personal lab notebook. A space to share hands-on projects, product insights, and the messy middle between technology and strategy. Each post will blend architecture, PM thinking, code experiments, and AI - written for product people who like to get their hands dirty. I believe there is no greater retention in learning than by building something: from idea to outcomes. Why I Started This Blog We’re entering a decade where product managers can’t just “talk tech” - they need to build with it. The ability to combine design, data, automation, platform, and architecture thinking will define the next generation of PMs. This blog is my way of staying hands-on - to keep learning, to share what works (and what doesn’t), and to build a public record of experiments at the frontier of AI and product design. Outside Work When I’m not experimenting with AI workflows or writing about platforms, I’m: building small learning projects with my 5-year-old son, reading and discussing Ramayana and Mahabharata, or helping my wife Gayathri, an urban designer, in her work on sustainable city projects across India. That balance between technology, learning, and purpose is what keeps me curious. How This Site Runs This blog is powered by Jekyll + Chirpy theme, hosted on GitHub Pages, and connected to my custom domain ujwaliyer.com. Everything here - from AI prototypes to product frameworks - is meant to be open, transparent, and reusable. Thanks for stopping by. If you’re working on something interesting or just want to swap ideas, reach me at ujwaliyer@live.com. Let’s learn, build, and ship better products - together. – Ujwal Iyer Senior Product Manager | SAP Labs India https://ujwaliyer.com" } ]
